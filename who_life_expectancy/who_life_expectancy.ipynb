{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e51b806-7f36-4530-b0d7-23acff5ee99f",
   "metadata": {},
   "source": [
    "# Introduction/Project Overview:\n",
    "\n",
    "In this notebook, I will present my solution and analysis of the Titanic dataset. This is a very famous dataset that can be found on kaggle. The dataset contains demographics of the Titanc passengers, incluiding who survived and who did not. The goal is to build a model that can correctly classify new examples (check who will survive or not). Throughout this notebook I will visualize the data, explain some data preprocessing techniques, construct and evaluate models and analyze the results.\n",
    "\n",
    "### Data Exploration & Preprocessing:\n",
    "I will go over the dataset, analyzing its various features, checking for missing values, and gaining insights into the distribution of variables. Prior to building the models, I will preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features to ensure good model performance.\n",
    "\n",
    "### Model Building & Evaluation:\n",
    "In this notebook I will try implement several models to try and correctly classify passengers who survied. This is a supervised learning task as we are given the labels of who survived and who did not. For this project the models I chose is random forests. Ideally I would test several to see which one performs best but to keep this simple as a practice example I will just use one model.\n",
    "\n",
    "\n",
    "### Conclusion:\n",
    "Finally, I will interpret the results of the models, identifying significant factors that contribute to passenger survival prediction and discussing potential areas for model improvement. The Titanic dataset is a good challange to test your knowledege on machine learning. This will serve as a good test for me to keep learning and testing my skills. The submission will be posted on kaggle to get a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b248c12e-b04c-4149-9126-4ff4d9abfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e8cac-a333-4f75-be18-627b0cb61ad8",
   "metadata": {},
   "source": [
    "## Data Exploration & Preprocessing:\n",
    "As mentioned earlier I got the dataset from kaggle. The link to that can be found above. The download came with two csv files. One for the training set and one for the test set. Since I have it locally on my computer I can eassily access the data as shown below. Some of the first steps we will do before creating a model is to see what our data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e048d651-d8b9-4487-9cc3-a19eb210c8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

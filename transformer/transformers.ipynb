{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064eab76-acab-474f-8f22-4ab7f046c1d6",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c0799-7158-4c69-a6fd-5ae447ef3c3c",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "In this project I will go over the transformer model architecture. Speciffically I will be explaining the concepts introduced in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762). This is a very important paper that changed not only the field of computer sceince but also non technical fields. The notebook will cover the architecture which includes encoder, decoder, attention, etc. I will also be covering the math behind it as well as training. Additionally I will also be implementing my own transformer using Jax. The code for that that can be found in the `./src`and `main.py`. Lets get into it! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21193d73-4f8f-4468-97b6-00f125958f26",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253199a-a50d-4619-bb5d-7df69dcdf0e5",
   "metadata": {},
   "source": [
    "The reason transofers are really good languge models are that they can proccess large ammounts of data at once. Previously LSTM networks or recurrent neural networks were used to model languge and although they worked they had lots of cons. For example previous language models had a hard time understanding data from beggining of sequences if the sequence was large, suffered from vanishing gradients, and processed data one token at a time. With the transformer architecture models are capable of better understanding data through the sequence and process large amounts of data at once. The specific task discussed in the paper was language translation. Which we will also try to implement. Usually for tasks like text generation only the decoder part of the model is used. Because I want to understand the transformer model thoroughly I want to implement the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f18250-41f9-45f0-a678-54caa12f09ce",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb69c1-5411-4f06-b981-d17bf4779deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea876190-e68d-49c6-8f26-5259ab64603b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cde3a0-dc7f-436c-8054-a2a57be6f966",
   "metadata": {},
   "source": [
    "Lets get into the encoder. First \n",
    "- Input is broken up into tokens, tokens are words or combinations of words\n",
    "- Each token is a vector where each vector holds some meaning for that token\n",
    "- Then the vectors are then passed through an attention block\n",
    "- Attention block handles the context of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8ad80-893b-43a2-a704-e4706b02d933",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999462d5-6433-48a3-be41-98ef5dd2cd50",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82698da3-8a3f-4248-9105-dabbfadd7be2",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b1d5c-db28-47de-9296-afd14d3208e2",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe579a-fc02-4560-9e6f-3ac798f97bfa",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25f567-7a94-41f8-b783-e48e17c91fa4",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679b14f-277a-4724-acea-15ba1dd4a513",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4e75e-34c6-4beb-a33e-7379211911a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

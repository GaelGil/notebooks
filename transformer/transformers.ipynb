{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "064eab76-acab-474f-8f22-4ab7f046c1d6",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c0799-7158-4c69-a6fd-5ae447ef3c3c",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "In this project I will go over the transformer model architecture. Speciffically I will be explaining the concepts introduced in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762). This is a very important paper that changed not only the field of computer sceince but also non technical fields. The notebook will cover the architecture which includes encoder, decoder, attention, etc. I will also be covering the math behind it as well as training. Additionally I will also be implementing my own transformer using Jax. The code for that that can be found in the `./src`and `main.py`. Lets get into it! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21193d73-4f8f-4468-97b6-00f125958f26",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253199a-a50d-4619-bb5d-7df69dcdf0e5",
   "metadata": {},
   "source": [
    "The reason transofers are really good languge models are that they can proccess large ammounts of data at once. Previously LSTM networks or recurrent neural networks were used to model languge and although they worked they had lots of cons. For example previous language models had a hard time understanding data from beggining of sequences if the sequence was large, suffered from vanishing gradients, and processed data one token at a time. With the transformer architecture models are capable of better understanding data through the sequence and process large amounts of data at once. The specific task discussed in the paper was language translation. Which we will also try to implement. Usually for tasks like text generation only the decoder part of the model is used. Because I want to understand the transformer model thoroughly I want to implement the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f18250-41f9-45f0-a678-54caa12f09ce",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb69c1-5411-4f06-b981-d17bf4779deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea876190-e68d-49c6-8f26-5259ab64603b",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cde3a0-dc7f-436c-8054-a2a57be6f966",
   "metadata": {},
   "source": [
    "Lets get into the encoder. The encoder part of the model is made up of \"N = 6 identical layers\". Essentially we repeat the encoder 6 times. The idea is that ...\n",
    "\n",
    "First the input is broken up into tokens, tokens are words or combinations of words and text\n",
    "Here is an example of some text. \n",
    "\n",
    "```\n",
    "lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod\n",
    "tempor incididunt ut labore et dolore magna aliqua. \n",
    "\n",
    "Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo\n",
    "consequat.\n",
    "```\n",
    "\n",
    "Here is some example of the text as tokens ie tokenized.\n",
    "\n",
    "\n",
    "```\n",
    "(lorem ipsum dolor) (sit) (amet, consectetur) (adipiscing elit, sed do) (eiusmod)\n",
    "(tempor) (incididunt) (ut labore) (et dolore magna aliqua.)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "For the purpose of this notebook we are going to represent each token as only just a word. Our embeddings are words/token representations. These input embeddings are learnable parameters of size 512 as defined in the paper. Each token is a vector that holds some meaning for that token. In the paper then token/vector size is of size 512. \n",
    "\n",
    "This is an example of a token. \n",
    "\n",
    "Dog = [12, 42, ... , 123]\n",
    "\n",
    "\n",
    "\n",
    "- After getting our input embedding we then apply positional encoding. We use $\\sin$ and $\\cos$ functions to apply to the token vector and add it.\n",
    "\n",
    "Here is an example.\n",
    "\n",
    "\n",
    "Positional encoding helps us track the location of the words and their importance there. This is calculated once. \n",
    "\n",
    "We then pass the embeddings into the multi head attention and also to a add and norm block. Lets go over the multi head attention mechanism. \n",
    "\n",
    "- Attention block handles the context of the words\n",
    "\n",
    "#### Attention\n",
    "Attention can be defined as \n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V $$\n",
    "This equations looks very confusing so lets break it down part by part. \n",
    "\n",
    "As mentioned we have token embeddings each of sive 512. Our Q matrix holds those embeddings\n",
    "Here is a visual of what that looks like\n",
    "\n",
    "Our K Matrix is also the token embeddings but transposed.\n",
    "\n",
    "Multiplying Q by $K^T$ we get this matrix. \n",
    "We then dvide this matrix by $\\sqrt(d_k)$ and apply softmax. \n",
    "\n",
    "Lastly we multiply all of that by our matrix V which is \n",
    "\n",
    "\n",
    "\n",
    "#### Multi Head Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8ad80-893b-43a2-a704-e4706b02d933",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999462d5-6433-48a3-be41-98ef5dd2cd50",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82698da3-8a3f-4248-9105-dabbfadd7be2",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b1d5c-db28-47de-9296-afd14d3208e2",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe579a-fc02-4560-9e6f-3ac798f97bfa",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e25f567-7a94-41f8-b783-e48e17c91fa4",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679b14f-277a-4724-acea-15ba1dd4a513",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4e75e-34c6-4beb-a33e-7379211911a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

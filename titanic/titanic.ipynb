{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf78193-1ff7-4014-b543-0e551925eb1e",
   "metadata": {},
   "source": [
    "# Introduction/Project Overview:\n",
    "In this notebook, I will present my solution and analysis of the Titanic dataset. This is a very famous dataset that can be found on [kaggle](https://www.kaggle.com/c/titanic/data). The dataset contains demographics of the Titanc passengers, incluiding who survived and who did not. The goal is to build a model that can correctly classify new examples (check who will survive or not). Throughout this notebook I will visualize the data, explain some data preprocessing techniques, construct and evaluate models and analyze the results. \n",
    "\n",
    "#### Data Exploration & Preprocessing:  \n",
    "I will go over the dataset, analyzing its various features, checking for missing values, and gaining insights into the distribution of variables. Prior to building the models, I will preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features to ensure good model performance.\n",
    "\n",
    "#### Model Building & Evaluation:\n",
    "In this notebook I will try implement several models to try and correctly classify passengers who survied. This is a supervised learning task as we are given the labels of who survived and who did not. For this project the models I have chosen are logistic regression, decision trees, random forests, support vector machines and neural networks. For each of these modules I will evalute their peformance using f1score, confusion matrices, and overall accuracy. \n",
    "\n",
    "#### Conclusion: \n",
    "Finally, I will interpret the results of the models, identifying significant factors that contribute to passenger survival prediction and discussing potential areas for model improvement. The Titanic dataset is a good challange to test your knowledege on machine learning. This will serve as a good test for me to keep learning and testing my skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1b053e-9c52-4894-9193-8b866509328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b158e7-821b-431f-8fc0-13096da8e727",
   "metadata": {},
   "source": [
    "# Data Exploration & Preprocessing:\n",
    "As mentioned earlier I got the dataset from kaggle. The link to that can be found above. The download came with two csv files. One for the training set and one for the test set. Since I have it locally on my computer I can eassily access the data as shown below. Some of the first steps we will do before creating a model is to see what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d789424-9326-4148-9060-9035945320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train and test sets\n",
    "train = pd.read_csv('./train.csv') \n",
    "test = pd.read_csv('./test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965fd96-94ed-4032-9d00-4a95e7a8dd8c",
   "metadata": {},
   "source": [
    "We loaded in the data into pandas dataframes and now we want to see what our data looks like. What does vairables does it contain and what data types, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb102a8-c07d-48ec-8014-051c71b59296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info() # get info on our train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc677df8-f056-4727-8ad0-a64d77b99c09",
   "metadata": {},
   "source": [
    "The block above gives us a lot of information. For starters it tells us we have a total of 891 samples. This is a good ammount as it is a big enohgh ammount for the model to learn from but not too large where we would require lots of computing power and time for training. We also see that we have 12 columns.`Age` has 177 missing values which is a big ammount. `Cabin` has a lot of missing values too. Lastly it seems we have 5 categorical columns and 7 numerical columns. \n",
    "\n",
    "Lets count our missing values and then look more into our data and see what will be useful for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f43ded-8e10-4fe4-b526-225919508224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cac03a-d326-4fb8-8186-2048b283db8a",
   "metadata": {},
   "source": [
    "As we saw `Age` has 177 missing values. `Cabin` has 2 however it might not be very useful when making a prediction on who survives the titanic but we will try to fill it. Lets breakdown each column, what they mean and if we will keep them for constructing our model. \n",
    "\n",
    "`PassengderId`: This column is not useful because its just a id assigned by the dataset. We will drop this column before training. \n",
    "\n",
    "`Survived`: This column is our labels and is important since this is a supervised machine learning problem. If we wanted to go with a clustering/unsupervised learning we could drop this. For the purpose of the notebook we will be keeping this. \n",
    "\n",
    "`Pclass`: On kaggle it says that this columns serves \"A proxy for socio-economic status\". Where 1 is upper, 2 is middle and 3 is lower. This will be important for our model\n",
    "\n",
    "`Name`: This is simply the name of the passenger. This could be important as some family last names could mean that they are from a wealthier family and therfore might have a higher chance of surviving. We can also use this to approximate age. For this notebook we will most likely drop it. \n",
    "\n",
    "`Sex/Age`: The sex of the passenger. Since women and children were  prioritized in the case of an emergency this would helpful to determined who would survive. \n",
    "\n",
    "`SibSp`: In the kaggle description of the dataset it says that sibsp is \"# of siblings / spouses aboard the Titanic\". This could be helpful. \n",
    "\n",
    "`Parch`: In the kaggle description of the dataset it says that parch is \"# of parents / children aboard the Titanic\". This will also be helpful. \n",
    "\n",
    "`Ticket`: Simply the ticket ID so we can remove this.\n",
    "\n",
    "`Fare`: How much they paid for their ticket. This can be useful as maybe workers did not pay for ticket and upper class people payed for their tickets. We will keep this column. \n",
    "\n",
    "`Cabin`: This is the cabin they were staying at. This could be useful but there are several missing values in this column so for now we will ignore it. \n",
    "\n",
    "`Embarked`: The Location of where they embarked. This could be important to determine who survived. For examples people who embarked at a certain location might be workers and others might be upper class familiies. This could be useful. \n",
    "\n",
    "Knowing how many missing values we have will be important because we will want to know if its worth filling or dropping. For example `Cabin` we will most likely drop because there is so many missing values. `Age` seems like a very important feature and not many are missing. Lets move on and get some information on the numerical columns that we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fda6ab-1f65-4a89-80d6-ee59ec7e1d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab816eee-5cb2-4836-8996-387d1f5bd4f0",
   "metadata": {},
   "source": [
    "The block above gives us a lot of information. For example we can see that the proportion of people that survived was `38%`. The mean age was `29`, the oldest person was 80 years old. Laslty average fare was `32.20`.\n",
    "\n",
    "Now lets compare `Age`,`SibSp`,`Parch`, and `Fare` by Survival to see any relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72fad4a4-44ea-46c6-9beb-d314739b2a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.626179</td>\n",
       "      <td>22.117887</td>\n",
       "      <td>0.329690</td>\n",
       "      <td>0.553734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.343690</td>\n",
       "      <td>48.395408</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age       Fare     Parch     SibSp\n",
       "Survived                                          \n",
       "0         30.626179  22.117887  0.329690  0.553734\n",
       "1         28.343690  48.395408  0.464912  0.473684"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(train, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066bfc9f-507e-4f3a-b867-7ea2ca529e7c",
   "metadata": {},
   "source": [
    "We can see that the average `Age` of people that did not survive was `30.6` while the average `Age` of those who did was `28.34`. Although not a huge difference it makes sense that people who survived are younger as they prioritized women and children in case of emergencies. Additionally a intersting relationship here is `Fare`. For those who did not survive their average `Fare` was `22.11` while those who did survive was `48.39`. This could mean that those who payed larger ammounts of money for their fare came from wealthy families and had priority. This tells us that `Fare` could be an important feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797ddcd-2a84-491c-9180-ba94bd538e79",
   "metadata": {},
   "source": [
    "Moving on lets do the same our categorical columns. Lets start by counting the number of survivors in `Pclass` which is socioeconomic class. We add `values=PassengerId` so that we count each passanger if we do not add that we count all the survivors in `pclass` within each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2384a3fc-2a49-4f8f-a757-5c5ad917a42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pclass</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>136</td>\n",
       "      <td>87</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pclass      1   2    3\n",
       "Survived              \n",
       "0          80  97  372\n",
       "1         136  87  119"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values='PassengerId', aggfunc ='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5eb75-0415-4f71-b938-449bfb9d1569",
   "metadata": {},
   "source": [
    "From the table above we see that most who died are in the lower class which is `Pclass` 3 with 372 people dead. Most survivors is `Pclass` 1 which is to be expected as that is the upper class. In the middle class we have a almost even number of survivors and dead people. This will for sure be helpful in determining who sill survive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b7d5f2-28a8-48f0-a076-0c1496489296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Sex</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Sex       female  male\n",
       "Survived              \n",
       "0             81   468\n",
       "1            233   109"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(train, index = 'Survived', columns = 'Sex', values='PassengerId', aggfunc ='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a2ff3-0cf7-4516-b89d-83a50d57285a",
   "metadata": {},
   "source": [
    "In this table above we see that most survivors were female. Most cassualties were male. This is pretty straight forward because women and children were prioritized in a emergency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4506a0c2-0922-4402-87f8-56611d3efe99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Embarked</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>47</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Embarked   C   Q    S\n",
       "Survived             \n",
       "0         75  47  427\n",
       "1         93  30  217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values='PassengerId', aggfunc ='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de881882-d4fa-4c65-8bd9-9c8bcd93827a",
   "metadata": {},
   "source": [
    "Lastly we have embarked whcih for the most part depending on location is relatively balanced. However in location `S` most people did not survive. This could be because maybe most lower class or workers embarked at this location. Either way it will be helpful in determining who will not survive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371c283-7a7f-460e-8363-caca9de32f52",
   "metadata": {},
   "source": [
    "Now that we have explored some of our data, lets fill in some missing values and work on some feature engineering. First we are going to combine the train and test. That way all the processing can be done at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd36ed20-77c0-4e40-9768-b8fa576b3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train_test'] = 1 # distinguish between sets \n",
    "test['train_test'] = 0\n",
    "test['Survived'] = np.NaN # fill survived with nan for test set\n",
    "data = pd.concat([train,test]) # join "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ffd85-14e5-4967-9c21-8f20895c577f",
   "metadata": {},
   "source": [
    "We previously saw that there are two missing values for `Embarked` in the training set. There. might be more in test set but either way lets fill them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35925bf2-3fbb-4ccb-aedd-b0d25dd88984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>train_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Icard, Miss. Amelie</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                       Name  \\\n",
       "61            62       1.0       1                        Icard, Miss. Amelie   \n",
       "829          830       1.0       1  Stone, Mrs. George Nelson (Martha Evelyn)   \n",
       "\n",
       "        Sex   Age  SibSp  Parch  Ticket  Fare Cabin Embarked  train_test  \n",
       "61   female  38.0      0      0  113572  80.0   B28      NaN           1  \n",
       "829  female  62.0      0      0  113572  80.0   B28      NaN           1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check embarked missing values\n",
    "data[data['Embarked'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f80607-c424-4056-b4ca-948b42f04929",
   "metadata": {},
   "source": [
    "There is still just two. These two people survived, are part of the upper class and are female. Lets get all the women who are upper class, fare of 80 or greater and survived. Then we can count where they embarked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fceecdfc-ee59-44b0-8874-676c220785bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_class_women_survived = data[(data['Sex'] == 'female') & (data['Pclass'] == 1) & (data['Fare'] >= 80) & (data['Survived'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31aa156b-4f8c-4388-aa53-3f8e27f6303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked\n",
       "C    24\n",
       "S    21\n",
       "Q     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_class_women_survived['Embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0f9d9-6ce2-465f-8087-096dbe68a910",
   "metadata": {},
   "source": [
    "The table abve says that women who were upper class, fare greater than 80 and survived embarked at `C` or `S`. Most survivors came from `S` with a total of 217. `C` has second most at `93`. With this information we will fill in the missing values witj `S`. This was a lot for 2 missing values but is good pracice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e20cc731-6f1b-4bd7-af26-4e94340e66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'] = data['Embarked'].fillna('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6789d6-7de3-4ac4-86e5-4f706dee2126",
   "metadata": {},
   "source": [
    "Lets move on to filling missing `Age` values. As we saw earlier there is a good amount of missing in the training set. To fill in the missing `Age` values we are going to be using the `Name` column. Most names contain a `Mr`, `Mrs`or some other thing in their name. These people would be older because they are married or have some sort of status. This could help us see if they have better chances of surviving. What we are going to do is get the average of age of each person with some sort of title in their name. The use those values to fill in the missing ones. For example we will calculate the average age of `Mrs` then we will use that average to fill in the age for any other `Mrs`.\n",
    "\n",
    "If they don't have that in their name and their `Parch` is zero then their age will be 0. This is because they are not a parent or child and in the kaggle datset info we are told that \"Some children travelled only with a nanny, therefore parch=0 for them.\"\n",
    "\n",
    "Lastly if they don't have `Mr` or `Mrs` in their name then we will get the avarage age of people who don't and use that for their missing `Age` value. First get each persons title and turn it encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d738644c-d675-4a7b-98fe-89b0a15062c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Name'] = data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5960718-7dd6-44c7-ba9e-70e6e1386638",
   "metadata": {},
   "source": [
    "Below we are encoding the columns `Pclass`, `Sex`, `Embarked` and `Name`. Name however has just the titles of people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fca5acb-9629-4900-a399-8821a2f5c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['Pclass', 'Sex', 'Embarked', 'Name'], prefix=['Pclass', 'Sex', 'Embarked', 'Name'], dtype=float) # encode the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1778f6-eba5-4c44-9844-8372f21690a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1309 entries, 0 to 417\n",
      "Data columns (total 35 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   PassengerId        1309 non-null   int64  \n",
      " 1   Survived           891 non-null    float64\n",
      " 2   Age                1046 non-null   float64\n",
      " 3   SibSp              1309 non-null   int64  \n",
      " 4   Parch              1309 non-null   int64  \n",
      " 5   Ticket             1309 non-null   object \n",
      " 6   Fare               1308 non-null   float64\n",
      " 7   Cabin              295 non-null    object \n",
      " 8   train_test         1309 non-null   int64  \n",
      " 9   Pclass_1           1309 non-null   float64\n",
      " 10  Pclass_2           1309 non-null   float64\n",
      " 11  Pclass_3           1309 non-null   float64\n",
      " 12  Sex_female         1309 non-null   float64\n",
      " 13  Sex_male           1309 non-null   float64\n",
      " 14  Embarked_C         1309 non-null   float64\n",
      " 15  Embarked_Q         1309 non-null   float64\n",
      " 16  Embarked_S         1309 non-null   float64\n",
      " 17  Name_Capt          1309 non-null   float64\n",
      " 18  Name_Col           1309 non-null   float64\n",
      " 19  Name_Don           1309 non-null   float64\n",
      " 20  Name_Dona          1309 non-null   float64\n",
      " 21  Name_Dr            1309 non-null   float64\n",
      " 22  Name_Jonkheer      1309 non-null   float64\n",
      " 23  Name_Lady          1309 non-null   float64\n",
      " 24  Name_Major         1309 non-null   float64\n",
      " 25  Name_Master        1309 non-null   float64\n",
      " 26  Name_Miss          1309 non-null   float64\n",
      " 27  Name_Mlle          1309 non-null   float64\n",
      " 28  Name_Mme           1309 non-null   float64\n",
      " 29  Name_Mr            1309 non-null   float64\n",
      " 30  Name_Mrs           1309 non-null   float64\n",
      " 31  Name_Ms            1309 non-null   float64\n",
      " 32  Name_Rev           1309 non-null   float64\n",
      " 33  Name_Sir           1309 non-null   float64\n",
      " 34  Name_the Countess  1309 non-null   float64\n",
      "dtypes: float64(29), int64(4), object(2)\n",
      "memory usage: 368.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info() # check our new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd1724-20dc-4d17-8fe7-248280f9a8cb",
   "metadata": {},
   "source": [
    "Now all that's left is to fill in the missing values for `Age`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b089136d-c8af-4f91-b3c5-012211d54b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_columns = ['Name_Capt', 'Name_Col', 'Name_Don', 'Name_Dona', 'Name_Dr', 'Name_Jonkheer', \n",
    "                'Name_Lady', 'Name_Major', 'Name_Master', 'Name_Miss', 'Name_Mlle', 'Name_Mme',\n",
    "                'Name_Mr', 'Name_Mrs', 'Name_Ms', 'Name_Rev', 'Name_Sir', 'Name_the Countess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c301d283-7e4a-4b3b-86c7-9298720714b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ages = {} # collect averages ages in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1df7742a-849e-480d-b576-d5927c4759a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_column in name_columns:\n",
    "    filtered_df = data[data[name_column] == 1]    \n",
    "    avg_age = filtered_df['Age'].mean()\n",
    "    average_ages[name_column] = avg_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4466c681-b35d-4c16-a054-4f003fc43e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name_Capt': 70.0,\n",
       " 'Name_Col': 54.0,\n",
       " 'Name_Don': 40.0,\n",
       " 'Name_Dona': 39.0,\n",
       " 'Name_Dr': 43.57142857142857,\n",
       " 'Name_Jonkheer': 38.0,\n",
       " 'Name_Lady': 48.0,\n",
       " 'Name_Major': 48.5,\n",
       " 'Name_Master': 5.482641509433963,\n",
       " 'Name_Miss': 21.774238095238097,\n",
       " 'Name_Mlle': 24.0,\n",
       " 'Name_Mme': 24.0,\n",
       " 'Name_Mr': 32.25215146299484,\n",
       " 'Name_Mrs': 36.99411764705882,\n",
       " 'Name_Ms': 28.0,\n",
       " 'Name_Rev': 41.25,\n",
       " 'Name_Sir': 49.0,\n",
       " 'Name_the Countess': 33.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1306b81-812e-44c7-8dda-26cb9cfea470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_age(df, mean_ages):\n",
    "    for index, row in df.iterrows():\n",
    "        if np.isnan(row['Age']):\n",
    "            name = row['Name'].split(',')[1].split('.')[0].strip()\n",
    "            print(name)\n",
    "            if name in mean_ages:\n",
    "                df.loc[index, 'Age'] = mean_ages[name]\n",
    "            elif row['Parch'] == 0:\n",
    "                df.loc[index, 'Age'] = 0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92fb4a0d-e05c-4a25-b70b-c00ac42a0942",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/git-repos/notebooks/titanic/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfill_age\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_ages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# fill the age column if nan\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m, in \u001b[0;36mfill_age\u001b[0;34m(df, mean_ages)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 4\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m mean_ages:\n",
      "File \u001b[0;32m~/git-repos/notebooks/titanic/env/lib/python3.9/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/git-repos/notebooks/titanic/env/lib/python3.9/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/git-repos/notebooks/titanic/env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Name'"
     ]
    }
   ],
   "source": [
    "fill_age(data, average_ages) # fill the age column if nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e91c0-2560-47b9-96c4-9f4610bad92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(train.loc[:, 'Age'].mean()) # fill missing values with average age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0df8b7-f45e-45af-b1f4-29dab009b188",
   "metadata": {},
   "source": [
    "Now lets change the name column. What we are going to do is strip the name and only keep the persons title. So stuff like `Mrs` will be the name if the have it. Once we have that we will encode the column.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d092ab-ea19-4078-9fcc-87362ea59ecb",
   "metadata": {},
   "source": [
    "We now have all the data that we need. We also got rid of the `Name` column because we don't really need it anymore. Below we are going to check and see if our dataset balanced. Because we are trying to classify `Survived` we need to make sure we have a good ammount of examples of each class `1` or `0`. If not our model will only learn from one of the classes more than the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a27e6-63f2-4262-a2ce-73a62778545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Survived'].value_counts() # seems slightly unbalanced but should be fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce5a13-33f9-4d1e-899b-65c26c166d80",
   "metadata": {},
   "source": [
    "Another step before we get to modeling is we need to encode the `Pclass`, `Sex`, and `Embarked`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bcc4b-a783-45cb-9d5f-4a706910e5d1",
   "metadata": {},
   "source": [
    "Now we should normalize the `Age` and `Fare` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2a8a4-3fc2-4ffe-9b48-fac410c1ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcf783-66f9-4eb8-b1d8-c269b8ebb2c6",
   "metadata": {},
   "source": [
    "We now have all the data as we want it. We encoded the `Pclass`, `Sex` and `Embarked` columns. We filled missing `Age` values. We have two dataframes to test if filling missing values was worth it. Now we need do apply the same transformations that we did to the training set to the test set. Then we can start modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87f7dc-5407-4daf-9f0d-d1d15b147a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "passenger_id = test.pop('PassengerId') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0cda7f-f62b-414c-b3b3-c0acf6f40403",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada9e805-313a-444e-9cd8-7eecf73adf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_age(test, mean_age_mrs, mean_age_mr) # fill the age column using average values from ealier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e568f4-6671-4ef5-9f54-070e16896bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(train.loc[:, 'Fare'].mean()) # since its just one missing value we will fill with mean of train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8a19d-57ed-4ac7-bff4-c7d2ddb98a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['Ticket', 'Cabin'], axis=1, inplace=True) # drop columns in test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99452af-5b06-47bf-9449-808994c3dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Name'] = test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777e3dbf-954a-4353-a5c0-f6f5b23199df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.get_dummies(test, columns=['Pclass', 'Sex', 'Embarked', 'Name'], prefix=['Pclass', 'Sex', 'Embarked', 'Name'], dtype=float) # encode the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5d933-4bdd-4e2e-9602-316aec79c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Age'] = scaler.fit_transform(X_test[['Age']])\n",
    "X_test['Fare'] = scaler.fit_transform(X_test[['Fare']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab513136-75d9-4535-90ef-d218b8762d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81cc7f-1bf2-4247-9168-86ecbf3b58dc",
   "metadata": {},
   "source": [
    "At this point I realised that we dont have labels for the test set. So I am just going to model and evaluate on the training dataset and leave the predictions for the test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cb4e03-d06f-455f-ba39-10e971ba1aed",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26749211-48a5-4240-9a4f-c8398be5ba1c",
   "metadata": {},
   "source": [
    "As mentioned in the project description, I was going to use logistic regression, random forests and neural networks for this dataset. But since there labels dont come with the test dataset I am just going to use random forests and a neural network since I think it will be the best peforming. I will then print the training accuracy and submit the predictions into kaggle. After that I will use whichever has a highest training accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0193946-4ede-4d4f-9e2e-9fc6c68ed17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ff485-af87-4c5b-853d-21441b5f041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.pop('Survived') \n",
    "X_train = train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29386ce1-4693-46d9-8a63-2b06a2b6244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42) # a classifer to use grid search on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef56847-2e19-48d5-b397-6655b755a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { # the parameters to test\n",
    "    'n_estimators': [100, 200],  \n",
    "    'max_depth': [10, 20, None],  \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    'min_samples_leaf': [1, 2, 4],  \n",
    "    'bootstrap': [True, False]  \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2b32de-4344-4301-a1b0-4e55da7503d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6c8d8-1620-47d8-a752-8ffb01806e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850fb1e-0213-4b9a-9213-8e604ad5da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_ # print best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88ac99-19ab-41af-b52d-c081694fdd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=None,\n",
    "                             bootstrap=True,\n",
    "                             min_samples_leaf=1,\n",
    "                             min_samples_split=10,\n",
    "                             random_state=42) # create the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba0e8e-9d42-4c70-adda-84afb806e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train) # fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb55b1-dd99-428b-9402-5ec07b754b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train) # getting predictions on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9760be-00a9-4ef2-86ad-b6fc627044f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_train, y_pred) # getting accuracy on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7756f-9dcd-4414-a26f-79f58d714691",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_train, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522aac6-f8e7-4e04-9d69-aaf06d92c0a0",
   "metadata": {},
   "source": [
    "The accuracy score is 0.91 which is not bad. Its not terribly low but not very high either so its not underfitting or overfitting. Now we can get the predictions and then just submit them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab1580-b5b4-4819-a23a-5e21df6a870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test) # predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a37e521-a8f5-4f62-b83d-646ff3e37ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': passenger_id.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54384def-d9d2-45d4-b46e-2292bfa5c9a5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "So ideally I would have checked the data in the `test.csv` to see that the labels are missing. Using the training set to evaluate is not really a good measurement. However seeing that it did not get a .99 on the accuracy might be a good thing because we know its not overfitting. To confirm this we could use the test if we had the labels. Some future improvments things for next time are to check both training and test set. So in the end this notebook serves more as a pandas excercise to get use to using all its features and was really helpful honestly. Additionally I got 0.77751 on kaggle for accuracy so not a terrible experiment.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7f3ef-3bf6-402e-b225-a4abf1a9efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what model am i selecting\n",
    "# why am i selecting the model\n",
    "# evaluate the results\n",
    "# use test data\n",
    "# possibly try another model\n",
    "# end with conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf78193-1ff7-4014-b543-0e551925eb1e",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "In this notebook, I will present my solution and analysis of the Titanic dataset.  This is a very famous dataset that can be found on [kaggle](). The objective is to build a model to correctly classify who will survive in from the titanic. . Throughout this notebook, I will meticulously explore the data, explain what im doing, construct and evaluate models, and critically assess the results.\n",
    "\n",
    "# Project Overview:\n",
    "The primary objective of this project is to delve into the Titanic dataset, which contains information about passengers, including their demographics and survival status. By carefully studying the data, I will identify meaningful patterns and relationships that could help me in predicting survival outcomes. To achieve this, I will follow a structured approach: \n",
    "\n",
    "**Data Exploration:** I will conduct a comprehensive exploration of the dataset, analyzing its various features, checking for missing values, and gaining insights into the distribution of variables.\n",
    "\n",
    "**Data Preprocessing:** Prior to building the models, I will preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features to ensure optimal model performance.\n",
    "\n",
    "**Model Building:** I will experiment with several machine learning algorithms, such as logistic regression, decision trees, random forests, and support vector machines, to construct predictive models based on the preprocessed data.\n",
    "\n",
    "**Model Evaluation:** To gauge the performance of the models, I will utilize appropriate evaluation metrics and techniques such as cross-validation to ensure robustness and mitigate overfitting.\n",
    "\n",
    "**Result Analysis:** Finally, I will interpret the results of the models, identifying significant factors that contribute to passenger survival prediction and discussing potential areas for model improvement.\n",
    "\n",
    "**Conclusion:**\n",
    "The Titanic dataset is a good challange to test your knowledege on machine learning. This will serve as a good test for me to keep learning and testing my skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1b053e-9c52-4894-9193-8b866509328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b158e7-821b-431f-8fc0-13096da8e727",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "As mentioned earlier I have got the dataset from kaggle. The link to that can be found above. The download came with two csv files. One for the training set and one for the test set. Since I have it locally on my computer I can eassily access the data as shown below. Some of the first steps we will do before creating a model is to see what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d789424-9326-4148-9060-9035945320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv') # read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965fd96-94ed-4032-9d00-4a95e7a8dd8c",
   "metadata": {},
   "source": [
    "Now that we have our data we want to see what it contains. First we will get its size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb102a8-c07d-48ec-8014-051c71b59296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10692"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.size # get size of train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc72df-53e9-4a01-a3d4-1c6814459878",
   "metadata": {},
   "source": [
    "As we can see it is somewhat large. This will be good for our model to have many samples to learn from. Additionally its not too large where we would require lots of compute power and time for training. Moving on from this we can check what our data consists of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33350197-6b3d-4d79-9d7c-e39534b597a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info() # get info on our train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a36da-c9aa-40aa-aab7-6f1b063ff11c",
   "metadata": {},
   "source": [
    "From the table above us we can gather a few things. First thing is that we have quite a big ammount of missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24798980-cd8d-44b7-b7a7-6baf11bb730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived         int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020e342-9795-4904-a019-16dbbd0ed9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

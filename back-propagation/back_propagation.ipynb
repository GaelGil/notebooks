{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274261cc-ff9f-4d9b-9f72-81ef0e65b5d7",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "In this notebook I will go over some of the basics of a fully connect feed forward neural network. I will mainly focus on applyling one to a problem, how neural networks are formed, trained, and the math behind them. To demonstrate this I will create my own neural network from scratch using numpy (a python library). In this notebook I will be using the mnist dataset (imported from tensorflow). In this dataset we are given a image a number (0-9) and its label (what number it is). I will go into more detail about the data later in the notebook. What we will do in this noktebook is create a neural network that takes a image of a hand written digit and classifies that image correctly.\n",
    "\n",
    "Now that we know what this notebook is goint to cover I have to mention some things that will be helpful to know before reading. This notebook goes into the math of neural networks. Some math knowledge of matrix multiplication and multivariable calculus will be useful. We will be doing all the math with numpy so python is important as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26541d35-ee3e-404c-8101-42409b2771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0d811-41c6-4de2-b752-98f50ab9484d",
   "metadata": {},
   "source": [
    "# The Data\n",
    "As I mentioned the data is the mnist dataset that we imported from tesorflow. We are given an image of a hand written digit (0-9) and its label. These are our X (digit) and y (label). The image of the digit is represented by a 28 by 28 matrix where each value of the matrix is a pixel in the image. The label is just simply a nuber. Below I have imported the data and split it into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ef9278-e2e4-4b07-bce1-aac11da8e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daff26-8c49-4228-98d4-052d8fd10bdb",
   "metadata": {},
   "source": [
    "Below is an image of a digit in our dataset. We can clearly see that it is a number five. As I mentioned this is simply just a 28 by 28 matrix where each value in the pixel. So the black parts are a value in the matrix (pixel) that is probably 0. However the actual number 5 probably has higher values like 200. The gray parts are somewhere in between those two values. One of the reasons for why we are using a neural network in to classify these digits is because.TODO: FINISH THIS. This dataset is very high in dimensions. That means that our X (input) has a lot of dimensions. Each input has a size of `28*28=784`. We cannot graph this dataset without using a dimensionality algoritm like PCA or TSNE to reduce the dimensions so we can graph it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbed065-7661-4726-ac66-76e677d503ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWG0lEQVR4nO3de5SU9X3H8fcneGlEFIlKkKgEYzVKIrGKiYdGPZQoRqsk1kqTI0YjaY40empNPOTES1usjaKJR5NCvBITg4kakZqIERVNlIoEb+D9koCriLgKeAW+/eN51gzLzjOzM7M74/4+r3P27Mx8n8t3np3PPLeZfRQRmFnf96FmN2BmvcNhN0uEw26WCIfdLBEOu1kiHHazRPTpsEuaIunyRg9by/DWfJL+S9Jp+e2DJS2rcrwTJN1b4zxrHreLaU2T9M2aJxARLfUD3AW8BmxZxXBvA6uBN4AHgTMrjVdlD8OAADbroed4dT79ozo9fnH++An5/RPy+9/uNNwy4OD89jnAtSW1o4DF+TJZCcwDPg78D7Am/3kXeK/k/m+66PFgYEPJMMuA64H9u/E8N+qtB18zFecD7AAsBz5c8vyWVTn9E4B7a+ytW+Pmf++1Jcv98pLaEODPwBa19NJSa3ZJw4C/JXvCf1/FKJMjYgDZQjgdOA64VZJ6rMnGeRI4vuOOpM2AY4FnOg23Cvi2pAGVJijpE8BMsmWxLVnILwPWR8Q/R8TWEbE1cB4wq+N+RIwrM8kX8+EHAJ8FHgfukTSmO0+0RZwA3BoRbzW7kSrsU/K3+XrHgxHRRvY3qCYbm2ipsJO9+O8nW/NNrHakiFgbEXeRLYTPAV8EkHSOpGs7hpN0vKQXJL0q6XuSnpf0d10MOz//3S5pjaTPdZ5n6fCShkkKSRMl/UnSSknfrdD2LcBoSdvl9w8DHgZe6jTcUuA+4F+rWBQjgeci4o7IrI6IGyLiT1WMW1Y+rWURcRZwOfDfHTVJP5T0Z0lvSHpQ0t/mjx8GTAH+MV+GD+WPf03SUkmrJT0r6Rsl09pe0hxJ7ZJWSbpH0ofy2k6SbpD0iqTnJH2raD5dGAfcXe45SjpT0jN5X0skjd90EF0q6XVJj5e+4UnaVtIVktokLZf0n5L6Vb2Au+cu8td3d7Vi2H+W/xwqaXB3Rs5f1AvJtg42Imkv4EfAV8i2BLYFhpaZ1Ofz3wPzd9f7qmxhNLAHMAY4S9InC4Z9G7iZbGsEsuc+s8yw3wNOkzSowvwXAXtKuljSIZK2rrLv7rgR2FdS//z+A2RvMoOAnwO/lPRXEfFbNt6C2CcffgVwBLAN8DXgYkn75rXTyXYXdgAGk4U48sDfAjxE9jcbQ7Y8Di2YT2efAp4oeF7PkL1utgXOBa6VNKSkfkA+zPbA2cCNJX+Pq4F1wCeAzwBfAL5OF/I3szML+gCYL+klSTfmW7ullgLlnmOhlgm7pNHArsD1EfEg2YL9pxom9SLZC6+zY4BbIuLeiHgXOItsd6GRzo2ItyLiIbIXZqU/ykzgeEkDgYOAX3c1UEQsBm4HvlM0sYh4lmxfdCjZ/vVKSVc3OPQvAgIG5vO8NiJejYh1ETEN2JLsDa9cj/8bEc/kWwt3A3P5y5vze2RvxLtGxHsRcU9kO6v7AztExL9HxLv58/wJf3mjrMZAsuM75fr6ZUS8GBEbImIW8BQwqmSQFcAP8r5mkb1xfDFfIR0OnJZvYa4gO/bSZW8RcUREnF/Q50Fkx4z2JFvWc/JdvA6r8+fSbS0TdrLN9rkRsTK//3O6sSlfYijZfm5nO5Ed3AAgIt4EXq1h+kVKN8HfBApDFhH3kq3FvgvMqbA/eRbwzUpbOxFxf0QcGxE7kIXo8/n0G2Uo2ZtkO4Ckf8s3y1+X1E62Zty+3MiSxkm6P99MbycLSsfwFwBPA3PzTfyONeCuwE755n17Pt4UsrV/tV4jO/ZQrq/jJS0umf6ITs9jef7G0+EFstfUrsDmQFvJuNOBHbvR2/siYn7+htYOnEp23KV0C3EA+bLvrs0qD9LzJH2Y7OBUP0kdgdkSGChpn3xNWc10dgb+hpJ9yhJtlKxx8nl+pMykevOrgNeSBfmQooEi4nFJN9KN4EbEA/k4I+prcSPjgUURsTbfP/822Wb1YxGxQdJrZGt+6LQcJW0J3EC2y3JzRLwn6dcdw0fEarJN+dMljQDmSXqA7E36uYjYvdxTraLvh4G/Jtvt2IikXcm2FMYA90XEekmLS54HwFBJKgn8LsDsvLd3gO0jYl0VfXRXdOrjk2Rbjd3WKmv2o4H1wF5k+38jyZ7UPZQcsS5H0laSDiLbB/4/4NYuBvsVcKSkAyVtQXa6ptxR+1fITjsN78ZzqNUlwFj+clCwyLlk+7kDuypKGi3pZEk75vf3JDtoeX89DSozVNLZZPuiU/LSALJ91VeAzSSdRbYv3uFlYFjHQTZgC7I38VeAdZLGke3fdsznCEmfkCTgdbLXxAayv+lqSd+R9GFJ/SSNkLR/mfl05VayTeSu9CcL1St5H19j0zfIHYFvSdpc0j+QvT5vzY+QzwWmSdpG0ock7Za/HrtF0t6SRubPb2tgGtnpwqUlgx0E/Ka704bWCftE4KqI+FNEvNTxA1wKfKXTPkupSyWtJvtj/4BsrXFYRGzoPGBEPAb8C/ALsrX8GrL9sHe6GPZNYCrw+3zT7LN1P8MyImJVx9HzKoZ9Dvgp2YuzK+1k4X5E0hrgt8BNwPdrbG+nfDpryNaInyI7vz83r9+Wz+NJss3atynZVQJ+mf9+VdKifM39LbLjCa+RHZOZXTL87sDv8vndB/woIu6MiPVkB/VGAs+RfX7gcrJdhk3mU+a5zAQOz7foNhIRS8iCdR/Za+lTwO87DbYg728l2WvjmIjo2A08nuyNbEn+vH5FduxhE5J+I2lKVzWy3ZJZZJ+ReJZs3/2IiHgvH3cI2Qrx12XGL6QqXmN9Uv7O2Q7snofI+jhJ5wErIuIHze6lFpKmAc9ExI9qGj+lsEs6EriDbPN9GtnplH2rWauafdC1ymZ8bzmK7HTGi2SbZMc56JaKpNbsZilLbc1ulqxePc8uyZsRZj0sIro8pVzXml3SYZKekPR0FZ/3NbMmqnmfPf9Wz5NkHwhZRnYedkJ+zrLcOF6zm/WwnlizjwKejohn8y+W/ILsaLeZtaB6wj6UjT8ttYwuvjIqaZKkhZIW1jEvM6tTjx+gi4gZwAzwZrxZM9WzZl8O7Fxy/2P5Y2bWguoJ+wPA7pI+nn+L7Dg2/lKDmbWQmjfjI2KdpMlk33zqB1yZf7PMzFpQr35c1vvsZj2vRz5UY2YfHA67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRJR8yWb7YOhX79+hfVtt922R+c/efLksrWtttqqcNw99tijsH7KKacU1i+88MKytQkTJhSO+/bbbxfWzz///ML6ueeeW1hvhrrCLul5YDWwHlgXEfs1oikza7xGrNkPiYiVDZiOmfUg77ObJaLesAcwV9KDkiZ1NYCkSZIWSlpY57zMrA71bsaPjojlknYEbpf0eETMLx0gImYAMwAkRZ3zM7Ma1bVmj4jl+e8VwE3AqEY0ZWaNV3PYJfWXNKDjNvAF4NFGNWZmjVXPZvxg4CZJHdP5eUT8tiFd9TG77LJLYX2LLbYorB944IGF9dGjR5etDRw4sHDcL3/5y4X1Zlq2bFlh/ZJLLimsjx8/vmxt9erVheM+9NBDhfW77767sN6Kag57RDwL7NPAXsysB/nUm1kiHHazRDjsZolw2M0S4bCbJUIRvfehtr76CbqRI0cW1ufNm1dY7+mvmbaqDRs2FNZPPPHEwvqaNWtqnndbW1th/bXXXiusP/HEEzXPu6dFhLp63Gt2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRPs/eAIMGDSqsL1iwoLA+fPjwRrbTUJV6b29vL6wfcsghZWvvvvtu4bipfv6gXj7PbpY4h90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwpdsboBVq1YV1s8444zC+hFHHFFY/+Mf/1hYr/QvlYssXry4sD527NjC+tq1awvre++9d9naqaeeWjiuNZbX7GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIvx99hawzTbbFNYrXV54+vTpZWsnnXRS4bhf/epXC+vXXXddYd1aT83fZ5d0paQVkh4teWyQpNslPZX/3q6RzZpZ41WzGX81cFinx84E7oiI3YE78vtm1sIqhj0i5gOdPw96FHBNfvsa4OjGtmVmjVbrZ+MHR0THxbJeAgaXG1DSJGBSjfMxswap+4swERFFB94iYgYwA3yAzqyZaj319rKkIQD57xWNa8nMekKtYZ8NTMxvTwRubkw7ZtZTKm7GS7oOOBjYXtIy4GzgfOB6SScBLwDH9mSTfd0bb7xR1/ivv/56zeOefPLJhfVZs2YV1itdY91aR8WwR8SEMqUxDe7FzHqQPy5rlgiH3SwRDrtZIhx2s0Q47GaJ8Fdc+4D+/fuXrd1yyy2F4x500EGF9XHjxhXW586dW1i33udLNpslzmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmifB59j5ut912K6wvWrSosN7e3l5Yv/POOwvrCxcuLFu77LLLCsftzddmX+Lz7GaJc9jNEuGwmyXCYTdLhMNulgiH3SwRDrtZInyePXHjx48vrF911VWF9QEDBtQ87ylTphTWZ86cWVhva2srrKfK59nNEuewmyXCYTdLhMNulgiH3SwRDrtZIhx2s0T4PLsVGjFiRGH9oosuKqyPGVP7xX6nT59eWJ86dWphffny5TXP+4Os5vPskq6UtELSoyWPnSNpuaTF+c/hjWzWzBqvms34q4HDunj84ogYmf/c2ti2zKzRKoY9IuYDq3qhFzPrQfUcoJss6eF8M3+7cgNJmiRpoaTy/4zMzHpcrWH/MbAbMBJoA6aVGzAiZkTEfhGxX43zMrMGqCnsEfFyRKyPiA3AT4BRjW3LzBqtprBLGlJydzzwaLlhzaw1VDzPLuk64GBge+Bl4Oz8/kgggOeBb0RExS8X+zx73zNw4MDC+pFHHlm2Vum78lKXp4vfN2/evML62LFjC+t9Vbnz7JtVMeKELh6+ou6OzKxX+eOyZolw2M0S4bCbJcJhN0uEw26WCH/F1ZrmnXfeKaxvtlnxyaJ169YV1g899NCytbvuuqtw3A8y/ytps8Q57GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRFb/1Zmn79Kc/XVg/5phjCuv7779/2Vql8+iVLFmypLA+f/78uqbf13jNbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwufZ+7g99tijsD558uTC+pe+9KXC+kc/+tFu91St9evXF9bb2or/e/mGDRsa2c4HntfsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiKp5nl7QzMBMYTHaJ5hkR8UNJg4BZwDCyyzYfGxGv9Vyr6ap0LnvChK4utJupdB592LBhtbTUEAsXLiysT506tbA+e/bsRrbT51WzZl8HnB4RewGfBU6RtBdwJnBHROwO3JHfN7MWVTHsEdEWEYvy26uBpcBQ4Cjgmnywa4Cje6hHM2uAbu2zSxoGfAZYAAyOiI7PK75EtplvZi2q6s/GS9oauAE4LSLekP5yOamIiHLXcZM0CZhUb6NmVp+q1uySNicL+s8i4sb84ZclDcnrQ4AVXY0bETMiYr+I2K8RDZtZbSqGXdkq/ApgaURcVFKaDUzMb08Ebm58e2bWKBUv2SxpNHAP8AjQ8Z3BKWT77dcDuwAvkJ16W1VhWklesnnw4OLDGXvttVdh/dJLLy2s77nnnt3uqVEWLFhQWL/gggvK1m6+uXj94K+o1qbcJZsr7rNHxL1AlyMDY+ppysx6jz9BZ5YIh90sEQ67WSIcdrNEOOxmiXDYzRLhfyVdpUGDBpWtTZ8+vXDckSNHFtaHDx9eS0sN8Yc//KGwPm3atML6bbfdVlh/6623ut2T9Qyv2c0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRCRznv2AAw4orJ9xxhmF9VGjRpWtDR06tKaeGuXNN98sW7vkkksKxz3vvPMK62vXrq2pJ2s9XrObJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolI5jz7+PHj66rXY8mSJYX1OXPmFNbXrVtXWC/6znl7e3vhuJYOr9nNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0RUc332nYGZwGAggBkR8UNJ5wAnA6/kg06JiFsrTCvJ67Ob9aZy12evJuxDgCERsUjSAOBB4GjgWGBNRFxYbRMOu1nPKxf2ip+gi4g2oC2/vVrSUqC5/5rFzLqtW/vskoYBnwEW5A9NlvSwpCslbVdmnEmSFkpaWF+rZlaPipvx7w8obQ3cDUyNiBslDQZWku3H/wfZpv6JFabhzXizHlbzPjuApM2BOcBtEXFRF/VhwJyIGFFhOg67WQ8rF/aKm/GSBFwBLC0Nen7grsN44NF6mzSznlPN0fjRwD3AI8CG/OEpwARgJNlm/PPAN/KDeUXT8prdrIfVtRnfKA67Wc+reTPezPoGh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLR25dsXgm8UHJ/+/yxVtSqvbVqX+DeatXI3nYtV+jV77NvMnNpYUTs17QGCrRqb63aF7i3WvVWb96MN0uEw26WiGaHfUaT51+kVXtr1b7AvdWqV3pr6j67mfWeZq/ZzayXOOxmiWhK2CUdJukJSU9LOrMZPZQj6XlJj0ha3Ozr0+XX0Fsh6dGSxwZJul3SU/nvLq+x16TezpG0PF92iyUd3qTedpZ0p6Qlkh6TdGr+eFOXXUFfvbLcen2fXVI/4ElgLLAMeACYEBFLerWRMiQ9D+wXEU3/AIakzwNrgJkdl9aS9H1gVUScn79RbhcR32mR3s6hm5fx7qHeyl1m/ASauOwaefnzWjRjzT4KeDoino2Id4FfAEc1oY+WFxHzgVWdHj4KuCa/fQ3Zi6XXlemtJUREW0Qsym+vBjouM97UZVfQV69oRtiHAn8uub+M1rreewBzJT0oaVKzm+nC4JLLbL0EDG5mM12oeBnv3tTpMuMts+xqufx5vXyAblOjI2JfYBxwSr652pIi2wdrpXOnPwZ2I7sGYBswrZnN5JcZvwE4LSLeKK01c9l10VevLLdmhH05sHPJ/Y/lj7WEiFie/14B3ES229FKXu64gm7+e0WT+3lfRLwcEesjYgPwE5q47PLLjN8A/Cwibswfbvqy66qv3lpuzQj7A8Dukj4uaQvgOGB2E/rYhKT++YETJPUHvkDrXYp6NjAxvz0RuLmJvWykVS7jXe4y4zR52TX98ucR0es/wOFkR+SfAb7bjB7K9DUceCj/eazZvQHXkW3WvUd2bOMk4CPAHcBTwO+AQS3U20/JLu39MFmwhjSpt9Fkm+gPA4vzn8ObvewK+uqV5eaPy5olwgfozBLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNE/D94/K7uquqPPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.title('A Digit in MNIST Dataset (label: 5)')\n",
    "plt.imshow(X_train[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5312eac5-221d-49ac-b836-1c7d0fab08a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0] # label of the digit above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d460c1c-8491-4260-b06f-0422016ff418",
   "metadata": {},
   "source": [
    "Now that we have taken a look at our data we can get into creating a neural network.\n",
    "\n",
    "# Neural Network Anatomy\n",
    "A neural network is formed by an input layer, some number of hidden layers, and an ouput layer. All of these layers contain some number of neurons. The input layer has as many neurons as it does inputs. For example in our network our input layer will have 784 neuros one for each pixel in the image. Similarly our output will layer will have ten neurons one for each number 1 through 9. In terms of hidden layers we can have as many as we choose but we also wan't to make sure we have a correct ammount. If we have too many or too little it could affect the performance of the neural network (same goes for neurons in the hidden layers). In the neural network built later in this notebook I choose to have two hidden layesr with 10 neurons. This might be a lot to just think about so below is an image of what the neural network we are building later will look like. \n",
    "\n",
    "# TODO add image of the neural network we are building\n",
    "The lines that are connecting the layers in the image above are called weights. These weights are what we are trying to learn this value of. We want to adjust them carefully and into a direction that gives us the lowest error when training. W can choose to set them randomly or to chnage them manually but that is not an efficient system. But now that we know what makes up a neural network and now that we know that we are trying to learn the values of these weights how do we even use them?\n",
    "\n",
    "To answer that question I will first go over feeding forward. Below I will explain how it is done. \n",
    "\n",
    "# The Math (Feeding Forward)\n",
    "Below I will go over feeding forward in a nueral network. Feeding forward is when we pass a input through the neural network to get an output. I will use the example of how the neural network we are buidling later will feed forward an input to get an output. \n",
    "\n",
    "First to feed foward the input layer to the first hidden layer we will multiply all inputs $\\vec{x}$ by all the weights connecting the input layer to h1 $W_1$ and add some bias $b_1$. Doing all that will give us $h_1$. We then apply some non linearity function to that and that will be our activations. In this case we are using relu. This looks like this:\n",
    "\n",
    "$$ \\vec{z_1} = relu(\\vec{h_1})$$\n",
    "$$ \\vec{h_1} = W_1 * \\vec{x} + \\vec{b_1} $$\n",
    "$$ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$$\n",
    "\n",
    "Now we just continue this proccess unitl we get to the output layer. So to keep forwarding this input we would now do this: \n",
    "\n",
    "$$ \\vec{z_2} = relu(\\vec{h_2})$$\n",
    "$$ \\vec{h_2} = W_2 * \\vec{z_1} + \\vec{b_2} $$\n",
    "$$ \\vec{z_2} = relu(W_2 * \\vec{z_1} + \\vec{b_2})$$\n",
    "\n",
    "Next we will do:\n",
    "$$ \\vec{h_3} = \\vec{h_3}$$\n",
    "$$ \\vec{h_3} = W_3 * \\vec{z_2} + \\vec{b_3} $$\n",
    "$$ \\hat{y} = \\vec{h_3}$$\n",
    "\n",
    "\n",
    "And lastly our ouput will simply just be:\n",
    "$$ \\hat{y} = softmax(\\vec{h_3}) $$ \n",
    "Softmax will turn all of the probabilities in the output layer to be values between 0 and 1. Now that we know how the feed forward function works we need to understand how to get the correct output we want when we pass in a input. We do this using back propagation which I will explain in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215472be-c1e3-4a5e-8b66-779f95dc32d4",
   "metadata": {},
   "source": [
    "# Back Propogation:\n",
    "The previous section went over how feed forward works in a neural network to get an output so we can classify (a image of a digit for example) something. \n",
    "As I mentioned neural networks contain weights and biases. We can manually adjust them or set them to random variables to see which values give us the best result. This of course is not a good proccess seeing how our network has so many weights and biases. What we want to do instead is learn the values to our weights and biases that will minimise our loss. We can do this with calculus. We can do this by taking the negative gradient of our cost function ($ C = (\\hat{y}-y)^2$). The gradient gives the direction of fastest assent. So if we take the negative gradient we will approach a place in which our loss is as close to 0 as possible.  Below I will explain the math that allows us to learn the values for our weights and biases. This will involved mutlivariable calculus.\n",
    "\n",
    "\n",
    "This is a basic example of what our network looks like. We have an input($X$), two hidden layers ($h_1,h_2$) and three weights ($W_1,W_2,W_3$) and our output layer $h_3$. \n",
    "$$ X ---W_1---> h_1 ---W_2---> h_2---W_3---> h_3$$\n",
    "\n",
    "To calculate the error that our network produces we use the cost function $ C = (\\hat{y} - y)^2 $\n",
    "where $ \\hat{y} $ is the output vector/activations and $ y $ is the label as a vector. An example of what these look like is:\n",
    "$$ \\hat{y} = \\begin{bmatrix}\n",
    "0.8 \\\\\n",
    "0.1 \\\\\n",
    "0.1 \n",
    "\\end{bmatrix} $$ and $$ y = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "Now that we know a little bit about or cost function we can move on. As we saw above to feed forward and input we use a series of functions. For example to get the first hidden layers activations we do this $ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$. What we want to do now is see how much each weight and bias is affecting our cost. In other words whats the derivative of the cost function $C$ with respect to $W_1$. Finding this out will tell us how much we should change the values by to minimize our cost. **Warning:** The following section is very math heavy. Some knowledge about multivariable calculus will be helpful.\n",
    "\n",
    "\n",
    "In the example below we will take the derivative of our cost function with respect to the second weights ($W_3$).\n",
    "$$ \\frac{\\partial C}  {\\partial W_3} = \\frac{\\partial C}  {\\partial \\hat{y}}*\\frac{\\partial \\hat{y}}  {\\partial W_3} = ? $$ \n",
    "\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "\n",
    "$$ \\hat{y} = \\vec{h_3} = W_3*\\vec{z_2}+b_3 \\hspace{10mm}  \\frac{\\partial \\hat{y}}  {\\partial W_3} = \\vec{z_2}$$ \n",
    "\n",
    "$$ \\frac{\\partial C}  {\\partial W_3} = \\frac{\\partial C}  {\\partial \\hat{y}}\\frac{\\partial \\hat{y}}  {\\partial W_3} = 2(\\hat{y}-y)\\vec{z_2}$$\n",
    "\n",
    "This will tell us by how much we should change the weights in $W_3$ Now we just continue this proccess until we reach the input layer. But first we need to calculate the error produced by the activations in the second hidden layer. So we need to find out. $\\frac{\\partial C}{\\partial h_2}$.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial h_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2}$$\n",
    "\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "\n",
    "$$ \\hat{y} = \\vec{h_3} = W_3*\\vec{z_2}+b_3 \\hspace{10mm}  \\frac{\\partial \\hat{y}}  {\\partial z_2} = W_3$$ \n",
    "\n",
    "$$ z_2 = relu(h_2) \\hspace{10mm} \\frac{\\partial z_2}  {\\partial h_2} = relu`(h_2)$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial h_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} = 2(\\hat{y} - y)W_3relu`(h_2)$$\n",
    "\n",
    "\n",
    "This will tell us the error produced by the activations in the second hidden layer. Now we continue our process of trying to find out how much the weights contributed to the error and how much we should move them by. Next we need to find $\\frac{\\partial C}{\\partial W_2}$.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial W_2} = ? $$ \n",
    "\n",
    "We know that $ \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} $ is equal to $ 2(\\hat{y} - y)W_3relu`(h_2)$\n",
    "So we just need to find $\\frac{\\partial h_2}{\\partial W_2}$\n",
    "$$h_2 = W_2*z_1+b_2 \\hspace{10mm}  \\frac{\\partial h_2}{\\partial W_2} = z_1$$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial W_2} = 2(\\hat{y} - y)W_3relu`(h_2)z_1 $$\n",
    "\n",
    "\n",
    "Our last two steps are more of the same but here they are. We need the find the error produced by the first hidden layer: The equation below will tell us. \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial h_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1} = ? $$\n",
    "\n",
    "Since we already know a good part of the equation all we need to find is:\n",
    "$$\\frac{\\partial h_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1} $$ \n",
    "\n",
    "$$ h_2 = W_2*z_1+b_2 \\hspace{10mm} \\frac{\\partial h_2}{\\partial z_1} = W_2$$\n",
    "\n",
    "$$ z_1 = relu(h_2)  \\hspace{10mm} \\frac{\\partial z_1}{\\partial h_1} =relu'(h1) $$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial h_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1} = 2(\\hat{y} - y)W_3relu`(h_2)z_1W_2relu'(h1)$$\n",
    "\n",
    "Lastly to find the error produced by the first weights we do calculation below:\n",
    "$$\\frac{\\partial C}{\\partial W_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial W_1} =?$$\n",
    "\n",
    "Simarly because we know what more than half of the equation is all we need to do is find \n",
    "$$\\frac{\\partial h_1}{\\partial W_1}$$\n",
    "\n",
    "$$h_1 = W_1*x+b_1  \\hspace{10mm}  \\frac{\\partial h_1}{\\partial W_1} = x$$\n",
    "$$\\frac{\\partial C}{\\partial W_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h_2} \\frac{\\partial h_2}{\\partial z_1} \\frac{\\partial z_1}{\\partial h_1}\\frac{\\partial h_1}{\\partial W_1} = 2(\\hat{y} - y)W_3relu`(h_2)z_1W_2relu'(h1)x $$\n",
    "\n",
    "That is all the math for the weights. We will be using these exact formulas to train our network. While that was a lot it becomes really easy to calculate derivatives once you have the first couple down. One thing I also wanted to mention was our network uses biases as well but I decided to not show the calculations for the biases because it is more repetition and they are very simple. However they are included in the code below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e28e26-ec59-4abd-946e-cb3863fe764a",
   "metadata": {},
   "source": [
    "Now that we have talked about how a neural network feeds forward an input to get and output. And that we have went over how we can learn the values of the weights and biases of our neural network, it is time to build one. Below is the code to a neural network class which we will train to classify dataset. The network below has an input layer of  784 neurons for inputs, 2 hiddeen layers with ten neurons and one output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4065e-a752-45ac-8563-97770ba9bd97",
   "metadata": {},
   "source": [
    "# Creating the Network\n",
    "Now that we know how we feed forward a input to get a output and how we learn the correct values for the weights in our neural network we can start to implement it. Below is a class representing a nueral network. As explained earlier the network will have a input layer, three hidden layers and three weights and biases. The feed forward function will do the exact same thing that was explained above. Same for the back propagation. I have comments on the code that explain what step we are calculating in feed forward or back propagation.inish explaining back propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8363207e-935e-4a20-b773-ea2469c6e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, h1_shape, h2_shape, output_shape):\n",
    "        # layers/activations\n",
    "        self.input = np.random.rand(input_shape, 1)- 0.5\n",
    "        self.h1 = np.random.rand(h1_shape, 1)- 0.5\n",
    "        self.h2 = np.random.rand(h2_shape, 1)- 0.5\n",
    "        self.h3 = np.random.rand(output_shape, 1)- 0.5\n",
    "        # weights \n",
    "        self.w_1 = np.random.rand(h1_shape, input_shape)- 0.5\n",
    "        self.w_2 = np.random.rand(h2_shape, h1_shape)-0.5\n",
    "        self.w_3 = np.random.rand(output_shape, h2_shape)- 0.5\n",
    "        \n",
    "        # biases \n",
    "        self.b_1 = np.random.rand(h1_shape, 1) - 0.5\n",
    "        self.b_2 = np.random.rand(h2_shape, 1) - 0.5\n",
    "        self.b_3 = np.random.rand(output_shape, 1) - 0.5 \n",
    "        \n",
    "    def relu(self,activations):\n",
    "        return np.maximum(0, activations)\n",
    "\n",
    "    def relu_deriv(self, activations):\n",
    "        return activations > 0\n",
    "\n",
    "    def softmax(self, activations):\n",
    "        return np.exp(activations) / np.sum(np.exp(activations))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # input\n",
    "        # reshapre the input into vector form.\n",
    "        # Example [1, 0, 0] -> [[1], [1], [1]]\n",
    "        self.input = np.reshape(X, (-1,1))\n",
    "        \n",
    "        # input -> h1\n",
    "        # the activations in the first hidden layer are given by the dot product \n",
    "        # of the weights by the input plus some biass its all then passed into\n",
    "        # our activation function. relu(W_1*x+b_1)\n",
    "        z1 = self.relu(np.dot(self.w_1, self.input) + self.b_1)\n",
    "        self.h1 = z1\n",
    "        \n",
    "        # h1 -> h2\n",
    "        # the activations in the seocnd hidden layer (h_2) are given by the dot product \n",
    "        # of the second weights (w_2) by the previous activations (h1) plus the bias(b_2).\n",
    "        # W_2*h1+b_2\n",
    "        z2 =  self.relu(np.dot(self.w_2, z1) + self.b_2)\n",
    "        self.h2 = z2 \n",
    "        \n",
    "        \n",
    "        z3 = np.dot(self.w_3, z2) + self.b_3\n",
    "        self.h3 = z3\n",
    "        # h2 -> output \n",
    "        # our output activtions/predictions are given by the second layer activations (h_2)\n",
    "        # put into the softmax function. \n",
    "        output = self.softmax(self.h3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def back_prop(self, output, y, learning_rate=0.01):\n",
    "        # error produced by the output\n",
    "        output_error = (2 * (output - y))\n",
    "\n",
    "        # derivative of cosf function with respect to third weights\n",
    "        dw_3 = -learning_rate * output_error.dot(self.h2.T)\n",
    "        # derivative of cost function with respect to the third biases\n",
    "        db_3 = -learning_rate * output_error\n",
    "        \n",
    "        # error produced by the second hidden layer activations\n",
    "        h2_error = self.w_3.T.dot(output_error) * self.relu_deriv(self.h2)\n",
    "        # derivative of cost function with respect to second weights\n",
    "        dw_2 = -learning_rate * h2_error.dot(self.h1.T)\n",
    "        # derivative of the cost function with respect to second biases\n",
    "        db_2 = -learning_rate * h2_error\n",
    "\n",
    "        # error produced by the first hidden layer activations\n",
    "        h1_error = self.w_2.T.dot(h2_error) * self.relu_deriv(self.h1)\n",
    "        dw_1 = -learning_rate * h1_error.dot(self.input.T)\n",
    "        db_1 = -learning_rate * h1_error\n",
    "        \n",
    "        # update all the weights\n",
    "        self.w_1 += dw_1\n",
    "        self.w_2 += dw_2\n",
    "        self.w_3 += dw_3\n",
    "        \n",
    "        # update all the biases\n",
    "        self.b_1 += db_1\n",
    "        self.b_2 += db_2\n",
    "        self.b_3 += db_3\n",
    "        return 0\n",
    "\n",
    "    def get_y_vector(self, y):\n",
    "        # return a vector that looks like [0, 0, 0, 1, ... 0] \n",
    "        vector = [1 if i == y else 0 for i in range(len(self.h2))]\n",
    "        vector = np.reshape(vector, (-1, 1))\n",
    "        return vector\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = self.feed_forward(np.array(X.flatten()))\n",
    "        return np.argmax(prediction)\n",
    "\n",
    "    def get_prediction(self, output_layer):\n",
    "        return np.argmax(output_layer)\n",
    "       \n",
    "    def accuracy(self, X_data, y_data):\n",
    "        correct = 0\n",
    "        for i in range(len(X_data)):\n",
    "            X = np.array(X_data[i].flatten())\n",
    "            y = self.get_y_vector(y_data[i])\n",
    "            output = self.feed_forward(X)  \n",
    "            prediction = self.get_prediction(output)\n",
    "            if np.argmax(y) == prediction:\n",
    "                correct += 1                \n",
    "        return correct/X_data.shape[0]\n",
    "    \n",
    "    def train(self, X_data, y_data, iterations):\n",
    "        for i in range(iterations):\n",
    "            for i in range(len(X_data)):\n",
    "                X = np.array(X_data[i].flatten())\n",
    "                y = self.get_y_vector(y_data[i])\n",
    "                output = self.feed_forward(X)\n",
    "                self.back_prop(output, y)\n",
    "        training_accuracy = self.accuracy(X_data, y_data)\n",
    "        return training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce48a95-b226-4103-b6a8-9e6ead4d5dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the shape of the input layer\n",
    "input_shape = X_train[0].flatten().shape[0]\n",
    "input_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ee308b-156d-4cce-9545-f6aff018feaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the shape of the output layer\n",
    "output_shape = len(list(set(y_train)))\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebe1891-f1c6-44c3-a134-f28066581067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the values in the image from 0-1\n",
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7785365b-6f70-4cb3-a184-e0d749e9daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(train_X, train_y, iterations):\n",
    "    accuracy_data = {'iteration': [], 'accuracy': []}\n",
    "    for i in range(len(iterations)):\n",
    "        nn = NeuralNetwork(input_shape=input_shape, h1_shape=10, h2_shape=10, output_shape=output_shape)\n",
    "        training_accuracy = nn.train(train_X, train_y, iterations[i])\n",
    "        accuracy_data['iteration'].append(iterations[i])\n",
    "        accuracy_data['accuracy'].append(training_accuracy)\n",
    "\n",
    "    accuracy_data = pd.DataFrame(data=accuracy_data)\n",
    "    return accuracy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af412b8-4803-4169-a4db-d45b61287206",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [i for i in range(1, 100, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542901cd-1d64-497e-9f5d-fc8d551e3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_info(X_train, y_train, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed73ce-e64c-4e83-8cac-8d3e4620545b",
   "metadata": {},
   "source": [
    "From the graph below we can see that 50 or even 10 iterations is just fine for trainig. After 50 the accuracy starts to go down which might be a sign of overfitting. Now that we have this information it is time to actually train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa49fb7-bcee-40cb-88c4-c24562ce9320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvrElEQVR4nO3debzc873H8ddbIiKLkCa2rFSiCSrqCK5SpVrU3hYpitrqlqrSli64aXW5VS2tKlW7UlVVt82lqO1aEyJnZCMiJBEEiS0iy/ncP76/I5NjTjJJzszvzJn38/GYR+a3f2YyZz7zXX7fryICMzOzltbKOwAzM2ufnCDMzKwkJwgzMyvJCcLMzEpygjAzs5KcIMzMrCQnCLM2Jml3SbPyjqOtSPpfSUfnHYdVnxOEVZ2k+yTNk7RO3rG0d5KulvTj7PlgSSGpcwWvd56k64vXRcQ+EXFNpa5p7ZcThFWVpMHArkAAB1T52hX7Yq0F9f76bdU5QVi1fQV4FLgaWK7aQtIASbdKmivpdUm/Ldp2gqTJkt6WNEnSJ7L1IWmLov2Kf3HvLmmWpO9Kehm4StIGkv6RXWNe9rx/0fG9JV0l6aVs+23Z+qcl7V+039qSXpO0XWsvVNL3sn1mSDoiW7eDpFckdSra7xBJE8p47x7I/p0v6R1JO2fHfzV7b+ZJulPSoKJzh6SvS3oWeDZbd5GkmZLekvSEpF2z9XsD3wMOy84/IVt/n6Tjs+drSfqBpBckvSrpWkm9sm3NJZyjJb2Yvfbvl/G6rJ1ygrBq+wpwQ/b4nKSNALIvzH8ALwCDgX7ATdm2LwHnZceuRyp5vF7m9TYGegODgBNJn/mrsuWBwHvAb4v2vw7oBmwFbAj8Klt/LXBk0X77AnMiYvwKrtsnex1HA5dL2jIixmaxf7Zo36Oy86/Mbtm/60dEj4h4RNKBpC/1Q4C+wIPAjS2OOwjYERieLY8FRpDelz8Bf5HUNSLuAH4C/Dk7/7YlYjgme3wa2BzowfLvH8AngS2BPYFzJA0r47VZexQRfvhRlQfpi2Mx0CdbngKcnj3fGZgLdC5x3J3Aaa2cM4AtipavBn6cPd8dWAR0XUFMI4B52fNNgCZggxL7bQq8DayXLd8CfKeVc+4OLAG6F627Gfhh9vy7wA3Z897AAmCTVs5V/HoGZ6+3c9H2/wWOK1peKzvfoKL3Z4+V/L/MA7bNnp8HXN9i+33A8dnze4D/LNq2ZfZ/2rkovv5F2x8HDs/7s+fH6j1cgrBqOhr4V0S8li3/iWXVTAOAFyJiSYnjBgDPreY150bEwuYFSd0kXZZVkbxFqrZZPyvBDADeiIh5LU8SES8BDwFfkLQ+sA+pFNSaeRHxbtHyC6QkA3A9sL+k7sChwIMRMWc1X98g4CJJ8yXNB94ARCq5NJtZfICkM7MqqTezY3qRSjvl2DR7Lc1eICWHjYrWvVz0fAGplGE1yI1WVhWS1iV9GXbK2gMA1iF9OW9L+hIbKKlziSQxE/hoK6deQKoSarYxUNzFtOVwxWeQfvXuGBEvSxoBjCd9qc4EektaPyLml7jWNcDxpL+bRyJidmuvF9hAUveiJDEQeBogImZLeoRULXQUcOkKzlOs1NDLM4HzI2JFyeqD47L2hu+Qqn8mRkSTpHmk19/aNYq9REpKzQaSSkuvAP1LHmE1yyUIq5aDgKWkevAR2WMYqc78K6SqiDnAzyR1l9RV0i7ZsVcAZ0raXskWRQ2xTwFfltQpa2T91Eri6Elqd5gvqTdwbvOG7Ff8/wK/yxqz15a0W9GxtwGfAE6jvDaD/5LUJftS3g/4S9G2a0lf1NsAt5ZxLkhVcE2kuv9mvwfOlrQVgKReWZtNa3qSvtDnAp0lnUNq12n2CjBYUmvfDTcCp0vaTFIPlrVZlCr5WY1zgrBqORq4KiJejIiXmx+kBs4jSL9g9we2AF4klQIOA4iIvwDnk6qk3iZ9UffOzntadtz87Dy3rSSOXwPrAq+RelPd0WL7UaQ69SnAq8A3mzdExHvAX4HNWPmX+sukuv2XSFVRX4uIKUXb/0b6Jf63iFiwknM1X38B6X14KKtS2iki/gb8HLgpqzJ7mlT91Zo7Sa/5GVL10EKWr4JqTmKvS3qyxPFXkhryHwCez44/tZz4rfYowhMGmZUr+8U9NCKOXOnOKz/Xc8BJEXH3mkdm1vbcBmFWpqxK6jhSKWNNz/UFUn3/v9f0XGaV4iomszJIOoFUFfO/EfHAyvZfybnuIzVMfz0imtogPLOKcBWTmZmV5BKEmZmV1GHaIPr06RODBw/OOwwzs5ryxBNPvBYRfUtt6zAJYvDgwYwbNy7vMMzMaoqkF1rb5iomMzMryQnCzMxKcoIwM7OSnCDMzKwkJwgzMyvJCcLMzEqqaIKQtLekqZKmSTqrxPZBku6R1JjNe9u/aP2Tkp6SNFHS1yoZp5mZfVjFEkQ2Q9clpKGHhwOjJA1vsdsFwLUR8XFgNPDTbP0cYOeIGEGaS/csSZtiZmbLufFG+NOfoBKjJlWyBDESmBYR0yNiEWkC+gNb7DOcZaNZ3tu8PSIWRcT72fp1KhynmVlNevVV+PrX4bLLKnP+Sn7x9mP5iUhmsfw8uQATSNMuAhwM9JT0EQBJAyQ1Zuf4eTYn8HIknShpnKRxc+fObfMXYNYeLFkCl1wCHijAWjrjDHjnHfj970Fa+f6rKu9f5mcCn5I0njRV5GzStJRExMys6mkL4GhJG7U8OCIuj4iGiGjo27fkUCJmNe2NN2CffeCUU2CnneC882Dx4ryjsvbgnnvg+uvhu9+FYcMqc41KJojZwICi5f7Zug9ExEsRcUhEbAd8P1s3v+U+pGkUd61grGbtzsSJsMMO8MADqQTx5S/Df/0X/Md/wNSpeUdneVq4EE4+GT76Ufje9yp3nUomiLHAkGxy8y7A4cDtxTtI6lM0OfrZpPlukdRf0rrZ8w2ATwL+k7C6cdttqcSwYAHcdx/853/CtdfCzTfD9Omw3Xbw299WpmHS2r+f/hSefRYuvRTWXbdy16lYgoiIJcAppEnSJwM3R8RESaMlHZDttjswVdIzwEakCdkBhgGPSZoA3A9cEBGFSsVq1l40NcHo0XDwwanaYNw42HnnZdu/9CUoFGC33eDUU2HvvWH27NbPZx3P1Knws5/BqFGw116VvVaHmVGuoaEhanm47ylT4DOfgdNPT4+18m4dsqp75x04+mi49VY46ii4/HLo2rX0vhGpYfKMM9I+l14Khx1W3Xit+iJgjz1g/Pj0nbHxxmt+TklPRERDqW3+Gmon/v3v9EvwzDPhgAPgtdfyjsiqafr01LZw221w4YVwzTWtJwdIPVZOPhmeegqGDIHDD4cjjoB586oVseXhuutSlePPf942yWFlnCDaiUIBevWC3/wG7roLRoxIjZPW8d1zT2qMnjUL7rgjlSDL7bI4dCg89FBqvP7zn2GbbeDuuysbr+Xj9ddTiXHnneGEE6pzTSeIdqKxMf1xn3IKPPoodOsGn/50qo9eujTv6KwSIuDii+Fzn0u/Bh9/fPXqlDt3hnPOSZ+bHj3SOb75TXjvvTYP2XL0ne+kEuLvf1+9KmgniHYgAp5+Gj7+8bS83XbwxBOpEercc9Mf/Jw5+cZobev99+G44+C002C//dKX+xZbrNk5GxrgySdT4/VFF8H226fPkdW+Bx+EK6+Eb31r2fdENThBtAMvvghvvZVKEM169kz1jVdeCY89Bttum6ofrPbNmQO77w5XXZV++d96a/r/bgvduqVSyb/+BW++mbrKnn9+uhvbatOiRXDSSTBoUPrBWE1OEO1AY2P6tzhBQKqHPvbY1NVxo43SHbXf/a7vpK1ljz+efukXCnDLLantoBLVBXvtla7xxS/CD34Au+4K06a1/XWs8i64ACZPTve9dO9e3Ws7QbQDhewOj623Lr192LD0xXLSSfDf/536wM+YUbXwrI1cc036v+vSBR5+GL7whcper3fvZSN9TpmSSqGXX+6b62rJc8/Bj36UPiv77Vf96ztBtAOFQio+9urV+j7rrpsap26+GSZNSu0Ut95avRht9S1ZknomHXNM6so6dmx165FHjUqfsf/4j/QjY//94eWXq3d9Wz0R6Q76tddObUp5cIJoBwqFD1cvteZLX0o3yWyxRfpVccopaVwWa59efz3d7fzrX8M3vgF33gl9+lQ/jv7907Uvvjh1q916a//AaO/+/OfUlvTjH0O/luNgV4kTRM7efz8V/8tNEACbb576vn/rW2kQt512gmeeqVyMtnqefhpGjlzWA+Wii9KvwbystVbq4fTkkzB4cPqBccwxqTHb2pf581NX5e23T/M95MUJImdTpqT7HFa1yqFLF/jlL+Ef/0g3WH3iE6nXk7UPf/vbssH27r8/dTZoL4YNg0cegR/+MH1mtt02xWjtx/e+B3PnpomAOnXKLw4niJw1N1CvSgmi2Oc/n4Zb2H57+MpX0i/Cd95pq+hsVTUPtnfIIbDVVqkH2k475R3Vh629dorzoYfS809/Gr797VSitXw9+mhqbzz11PR3nScniJw1NqY/0KFDV/8c/funeuVzzklDQjc0LOs6a9Xzzjupjejcc1Oyvv/+/OqOy7XTTukHxkknpe6UO+zgz06eFi9O/xebbpp6L+XNCSJnhUIq8q9p3XTnzqlP/d13pzrlkSPTr5Ba7NK4cCGMGZOqP55/vjZew/TpaYyc226DX/0Krr56xYPttSfdu6fRYP/5z1St0dCQulN7iJfqu+iilKAvvrjtbp5cIxHRIR7bb7991KJ+/SKOPLJtz/nKKxGf+1wERHzxixHz5rXt+Svhtdcirrkm4pBDIrp3T7E3PwYMiDjqqIgrroiYNi2iqSnvaJd3990RvXtHbLBBxL/+lXc0a2bu3IgvfCG977vuGjF9et4R1Y8ZMyK6dYvYb7/qfsaBcdHK92ruX+xt9ajFBPH66+l/4Oc/b/tzL12aztupU8TgwRGPPdb211hTzz0XceGFEZ/6VIoTIjbZJOKkkyLGjImYMCHiN79JSa5v32UJo1+/iC9/OeLyyyOmTs0vYTQ1Rfz61yn24cMjnn02nzjaWlNTxLXXRqy3XkSPHhFXXtn+knJH09QUsf/+KUHMmFHdaztBtFP33Zf+B8aMqdw1Hn44YtCgiM6dIy64ICWOvCxdmhLV978fsfXWy77wt946rXv88dbja2qKmDgx4ne/izj00IiNNlp2/CabRBx+eMSll0ZMnlydL7P33os45ph0/QMPjHjrrcpfs9pmzIjYffdlr/GVV/KOqOO69db0Pv/iF9W/dm4JAtibNJf0NOCsEtsHAfcAjcB9QP9s/QjgEWBitu2wlV2rFhPEb36T/gdmzarsdd54I+Lgg9O19t034tVXK3u9YgsXpgR40kkRm26aYlhrrVRquPDCVIpYHU1NEVOmRPz+9xGjRi07N6Tk8aUvRVxyScTTT7d9wpg9O2LHHdO1zjkn36RbaUuXpv+nddaJ2HDDiNtvzzuijuett1Kp+OMfj1i0qPrXzyVBAJ2A54DNgS7ABGB4i33+AhydPd8DuC57PhQYkj3fFJgDrL+i69VigjjxxFRvXY1fvE1NEb/9bUSXLunL9L77Knet11+PuO66VDXUo0f6lHXvnuq2r7kmtTe0taamiGeeifjDH1KbTv/+yxJGnz7p2hdfHNHYuGZf6I8+mkos3btH/PWvbRd/e1coRGy7bXo/d989JYqOnBir6bTTIqSIRx7J5/p5JYidgTuLls8Gzm6xz0RgQPZcwFutnGtCc8Jo7VGLCWKnnSJ226261xw/PmLo0PQr/rzzIpYsaZvzTp8e8atfpS+P5vaEjTdOSfCf/0xVMtXU1JRKJ3/8Y8RXvhIxcOCyhNG7d8RBB6X2g/Hjy/+iu+qqlGA32ywlmnrz/vsRv/xl6jQAEUOGpFLaO+/kHVnteuKJ9Ld48sn5xZBXgvgicEXR8lHAb1vs8yfgtOz5IUAAH2mxz0hgMrDWiq5Xawli6dL06/rrX6/+td96K/UKav41OHv2qp+jqSli7NiIH/wgYpttln35brVVxNlnp1/a7e0X5vPPR1x9dcSxx6Yv+eaY118/4oAD0pffuHEfTpqLF6dfeRCxxx6VKQHVksWLI266KWLkyPSebLBBxFlnVb6qtKNZsiSioSH9kMqzp2F7ThCbArcC44GLgFnFVUnAJlkbxk6tXONEYBwwbuDAgZV7Bytg+vT07l92WX4xXH116jXRp095DeULF0bccUf6tdOvX3zQnrDbbunLtdZ68bz4YqoKO+64iC22WJYwevWK+PznI/77vyMeeCBizz3T+tNOS1+OljQ1RTz0UKpKXGut1BHiiCNSkrWVu/ji9Lm68cZ842i3VUwt9u8BzCpaXg94EvhiOdertRLE3/+e3v2HH843jkmTlpUAvv3tDzeSvfFGxPXXp0bfnj3Tft26pUbvq69O/eY7ilmzIm64IVWLDR26LGF06ZK6elrrpk+P+OY3l31Gdtst4rbb2q4Ks6OZNSu9V5/9bP5diPNKEJ2B6cBmRY3UW7XYp09z1RFwPjA6e94l6930zXKvV2sJ4kc/Su9+e+geuWBBxNe+luLZccf0q/Cii1J1SufO8UHPoOOPj/if/0n714OXXor4y19SA62VZ/78VJocNCh9bj760dRb7+23846sffnCFyK6dk03fuYtlwSRrsu+wDNZb6bvZ+tGAwdkz78IPJvtcwWwTrb+SGAx8FTRY8SKrlVrCeLQQ1M9eHty883p5qjmX87DhqW65UceaX/tCda+LV6cPk877xwftPN85zupWq/e/eMf6T05//y8I0lWlCCUtte+hoaGGDduXN5hlG34cBgyBP7+97wjWd6MGWngv912S/GZralHH03jU91yS5pn/dBD0wx7O+yQd2TV9+67aZTf7t3TxF9duuQdEUh6IiIaSm3zYH05WLgwTfCzukN8V9LgwXDccU4O1nZ22inNjjZ9epoE55//TINJ7rprmjejngYFHD0aXnghDaTZHpLDyjhB5GDy5PRH0R4ThFmlDBqUhhSfOTOVKGbNSvNmDB2aRjF9++28I6ysQgEuvBC++tWUHGuBE0QOmicJqubE9WbtxXrrpZLEtGmp2mmTTdJy//5w5pnw4ot5R9j2mprgxBNh/fXTUOq1wgkiB4UCrLOOq3GsvnXqlObF/r//g8ceg333hV//Os25fvjhaV1H8Yc/pLaYX/4SPvKRvKMpnxNEDhob0yRBnTvnHYlZ+zByJNx4Y2qn+Na34I47UtvFLrukUsaSJXlHuPpeeQXOOitN63rUUXlHs2qcIHJQKLh6yayUgQNTFczMmWlWtZdfTtO4DhmS2i3eeivvCFfdt74FCxakWfukvKNZNU4QVfb66zBnjhuozVakZ0849dTU2+9vf4MBA9IXbf/+6d8ZM/KOsDx33QV/+lMqQWy5Zd7RrDoniCprbqB2gjBbuU6d4KCD4IEHYOxY2H9/+M1v4KMfhQMPhDvvTA3A7dF778HJJ6fSz9ln5x3N6nGCqLLGxvSvE4TZqmlogBtugOefh+9+NzX67r136iZ7wQWpdN6e/OQn8NxzqWqpa9e8o1k9ThBVViikXgybbJJ3JGa1qX//9OU7c2Zq2N50U/j2t6FfPzj66NT7Ke8BIiZPhp//HI48EvbcM99Y1oQTRJUVCqn0UGuNVWbtTZcuqTvsAw+kkvlxx8Gtt6beTw0NcMUVaWiLaotIVUs9eqRurbXMCaKKmprg6addvWTW1rbZBi65BF56CX73O1i8GE44IZUqTjsNpkypXizXXAP3359KEBtuWL3rVoITRBU9/3z6ReMurmaV0bNn+vU+YQI8+GC6+e7SS9N9R3vume6pWLy4ctd/7bV0N/guu6QSTa1zgqgi92Ayqw4JPvnJ1MV05szUZjFtWrqnYtAgOO88mD277a/77W/Dm2/CZZfBWh3g27UDvITa0Zwgttoq3zjM6slGG6VuptOnw//8D4wYkUZVHTQoDfVx991t06h9//1w9dWpBNFR/sadIKqosTGNM9OjR96RmNWfTp1gv/1gzJhUmjjjjPSlvtde8LGPpXGg5s1bvXO//z587Wuw2Wbwwx+2adi5coKoIg+xYdY+bL55akSeNQuuvTZ1PT/99NSofdxx8MQTq3a+X/wiNYRfcgl061aZmPPgBFEl770Hzz7r9gez9qRr1zSA3sMPw5NPpvsWbropdZPdccdUZfTeeys+x7Rp8OMfp5ny9tmnKmFXTUUThKS9JU2VNE3SWSW2D5J0j6RGSfdJ6l+07Q5J8yX9o5IxVsukSambqxOEWfu03XZw+eWpq+zFF6eBAY89NpUqzjwzJYKWmu95WGedVEXV0VQsQUjqBFwC7AMMB0ZJGt5itwuAayPi48Bo4KdF234B1NjguK3zJEFmtaFXrzRQ4KRJcO+98JnPpBnvhgyBz30Obrtt2fDjN96YGrl/8pOOOTpCJUsQI4FpETE9IhYBNwEHtthnOPDv7Pm9xdsj4h6gw0xCWCik4uwWW+QdiZmVQ4Ldd4ebb06z3I0eDRMnwsEHp8bo0aNTu8UOO6QG6o6okgmiHzCzaHlWtq7YBOCQ7PnBQE9JZc+3JOlESeMkjZs7d+4aBVtphQIMH556UphZbdlkk9Q7acaMNJzHsGFw7rnpxrjLLuu4f9d5N1KfCXxK0njgU8BsYGm5B0fE5RHREBENffv2rVSMbaKx0e0PZrWuc+dUgvjXv2DqVHjoodR20VFVctLL2cCAouX+2boPRMRLZCUIST2AL0TE/ArGlIu5c9O0g25/MOs4hg7NO4LKq2QJYiwwRNJmkroAhwO3F+8gqY+k5hjOBq6sYDy58RAbZlaLKpYgImIJcApwJzAZuDkiJkoaLemAbLfdgamSngE2As5vPl7Sg8BfgD0lzZL0uUrFWmmeJMjMalElq5iIiDHAmBbrzil6fgtwSyvH7lrJ2KqpUIC+fdOYMGZmtSLvRuq64EmCzKwWOUFU2NKlqe+0q5fMrNY4QVTY9OmwYIEThJnVHieICvMQG2ZWq5wgKqxQSG0PHWUCETOrH04QFVYowEc/2rHGiDez+uAEUWGNja5eMrPa5ARRQQsWpDHk3UBtZrXICaKCJk1KE4o4QZhZLXKCqCAPsWFmtcwJooIKBVh33dRIbWZWa5wgKqhQSN1bO+pkImbWsTlBVFDzGExmZrXICaJCXnkFXn3VXVzNrHY5QVSIJwkys1rnBFEhThBmVuucICqksRE23DA9zMxqUUUThKS9JU2VNE3SWSW2D5J0j6RGSfdJ6l+07WhJz2aPoysZZyUUCm5/MLPaVrEEIakTcAmwDzAcGCVpeIvdLgCujYiPA6OBn2bH9gbOBXYERgLnStqgUrG2NU8SZGYdQVkJQtKtkj4vaVUSykhgWkRMj4hFwE3AgS32GQ78O3t+b9H2zwF3RcQbETEPuAvYexWunavnnoOFC50gzKy2lfuF/zvgy8Czkn4macsyjukHzCxanpWtKzYBOCR7fjDQU9JHyjwWSSdKGidp3Ny5c8t7JVXQPMSGq5jMrJaVlSAi4u6IOAL4BDADuFvSw5KOlbT2Glz/TOBTksYDnwJmA0vLPTgiLo+Ihoho6Nu37xqE0bYKBVhrLRjeskLNzKyGlF1llP2yPwY4HhgPXERKGHe1cshsYEDRcv9s3Qci4qWIOCQitgO+n62bX86x7VmhAFtskcZhMjOrVeW2QfwNeBDoBuwfEQdExJ8j4lSgRyuHjQWGSNpMUhfgcOD2FuftU9SucTZwZfb8TuCzkjbIGqc/m62rCY2Nbn8ws9pXbgni4ogYHhE/jYg5xRsioqHUARGxBDiF9MU+Gbg5IiZKGi3pgGy33YGpkp4BNgLOz459A/gRKcmMBUZn69q9d9+F6dPd/mBmta9zmfsNlzQ+q/4h+1U/KiJ+t6KDImIMMKbFunOKnt8C3NLKsVeyrERRMyZO9CRBZtYxlFuCOKE5OQBkXU9PqEhENc5DbJhZR1FugugkSc0L2U1wXSoTUm1rbIRu3WDzzfOOxMxszZRbxXQH8GdJl2XLJ2XrrIVCAbbeOnVzNTOrZeUmiO+SksLJ2fJdwBUViaiGRaQEcWDL+8XNzGpQWQkiIpqAS7OHteKVV+C119z+YGYdQ1kJQtIQ0kB6w4GuzesjwjXtRTzEhpl1JOXWlF9FKj0sAT4NXAtcX6mgapV7MJlZR1Juglg3Iu4BFBEvRMR5wOcrF1ZtKhRg442hT5+8IzEzW3PlNlK/nw2J8aykU0jjIrU2xEbdamx09ZKZdRzlliBOI43D9A1ge+BIoOZmeaukJUtg0iRXL5lZx7HSEkR2U9xhEXEm8A5wbMWjqkHTpsH77ztBmFnHsdISREQsBT5ZhVhqmhuozayjKbcNYryk24G/AO82r4yIWysSVQ1qbPQkQWbWsZSbILoCrwN7FK0LwAkiUyjA0KHQtevK9zUzqwXl3kntdoeVKBRg++3zjsLMrO2Ueyf1VaQSw3Ii4qttHlENevvtNEnQsU6jZtaBlFvF9I+i512Bg4GX2j6c2jRxYvrXDdRm1pGUdR9ERPy16HEDcChQcqrRYpL2ljRV0jRJZ5XYPlDSvZLGS2qUtG+2voukqyQVJE2QtPuqvazqcg8mM+uIVnfWgiHAhivaIbt/4hJgH9Igf6Mktezj8wPSXNXbAYcDzVOYngAQEdsAewG/zO7kbpcKBejeHQYPzjsSM7O2U24bxNss3wbxMmmOiBUZCUyLiOnZOW4CDgQmFe0TwHrZ814sq7YaDvwbICJelTSfVGJ5vJx4q62xMZUePEmQmXUk5VYx9YyI9YoeQyPirys5rB8ws2h5Vrau2HnAkZJmAWOAU7P1E4ADJHWWtBlpeI8BLS8g6URJ4ySNmzt3bjkvpc01TxLk6iUz62jKShCSDpbUq2h5fUkHtcH1RwFXR0R/YF/guqwq6UpSQhkH/Bp4GFja8uCIuDwiGiKioW/fvm0QzqqbMwfeeMMJwsw6nnIrRc6NiDebFyJiPnDuSo6ZzfK/+vtn64odB9ycnfMRUg+pPhGxJCJOj4gREXEgsD7wTJmxVpUnCTKzjqrcBFFqv5W1X4wFhkjaTFIXUiP07S32eRHYE0DSMFKCmCupm6Tu2fq9gCURMYl2yD2YzKyjKvc+iHGSLiT1SgL4OvDEig6IiCXZ3BF3Ap2AKyNioqTRwLiIuB04A/iDpNNJDdbHRERI2hC4U1ITqdRx1Cq/siopFGDTTaF377wjMTNrW4r40A3SH94p/Zr/IfAZ0hf5XcD5EfHuCg+sooaGhhg3blzVr7vddrDRRnDHHVW/tJnZGpP0RESUvK+t3LGY3gU+dKNbvVu8OE0StNdeeUdiZtb2yu3FdJek9YuWN5B0Z8WiqhHPPguLFrn9wcw6pnIbqftkPZcAiIh5rORO6nrgBmoz68jKTRBNkgY2L0gaTInRXetNYyN06gTDhuUdiZlZ2yu3F9P3gf+TdD8gYFfgxIpFVSMKBdhyS1hnnbwjMTNre+UOtXEHaSykqcCNpO6p71UwrprgITbMrCMrd7C+44HTSHdDPwXsBDzC8lOQ1pW33oIZM+D44/OOxMysMsptgzgN2AF4ISI+DWwHzK9UULXg6afTvx5iw8w6qnITxMKIWAggaZ2ImAJsWbmw2j/3YDKzjq7cRupZ2X0QtwF3SZoHvFCpoGpBoQA9e8KgQXlHYmZWGeXeSX1w9vQ8SfeSJvep68ElmhuopbwjMTOrjHJLEB+IiPsrEUgtiUj3QBx2WN6RmJlVjifJXA2zZ8P8+W5/MLOOzQliNbiB2szqgRPEamieRc4Jwsw6MieI1VAoQP/+sMEGeUdiZlY5ThCrwUNsmFk9qGiCkLS3pKmSpkn60IRDkgZKulfSeEmNkvbN1q8t6RpJBUmTJZ1dyThXxeLFMHmy76A2s46vYglCUifSHNb7AMOBUZKGt9jtB8DNEbEdcDjwu2z9l4B1ImIbYHvgpGyI8dxNnZqShEsQZtbRVbIEMRKYFhHTI2IRcBNwYIt9Algve94LeKlofXdJnYF1gUXAWxWMtWzuwWRm9aKSCaIfMLNoeVa2rth5wJGSZgFjgFOz9bcA7wJzgBeBCyLijZYXkHSipHGSxs2dO7eNwy+tUIDOneFjH6vK5czMcpN3I/Uo4OqI6A/sC1wnaS1S6WMpsCmwGXCGpM1bHhwRl0dEQ0Q09O3btyoBNzam5NClS1UuZ2aWm0omiNnAgKLl/tm6YscBNwNExCNAV6AP8GXgjohYHBGvAg+RJizKnXswmVm9qGSCGAsMkbSZpC6kRujbW+zzIrAngKRhpAQxN1u/R7a+O2mCoikVjLUsb74JL77oBGFm9aFiCSIilgCnAHcCk0m9lSZKGi3pgGy3M4ATJE0gTWV6TEQEqfdTD0kTSYnmqohorFSs5fIkQWZWT1Z5NNdVERFjSI3PxevOKXo+CdilxHHvkLq6tiseYsPM6knejdQ1pVCAXr1gwICV72tmVuucIFZBoQBbb+1JgsysPjhBlCkiJQi3P5hZvXCCKNPMmakXk9sfzKxeOEGUyUNsmFm9cYIokxOEmdUbJ4gyNTbCwIGpF5OZWT1wgiiTh9gws3rjBFGGRYtgyhQnCDOrL04QZZg6FZYscRdXM6svThBl8BAbZlaPnCDKUCjA2mvDllvmHYmZWfU4QZShUIBhw1KSMDOrF04QZWhsdPWSmdUfJ4iVmDcPZs1ygjCz+uMEsRLNkwQ5QZhZvXGCWInmITbcxdXM6k1FE4SkvSVNlTRN0lkltg+UdK+k8ZIaJe2brT9C0lNFjyZJIyoZa2saG2H99aFfvzyubmaWn4olCEmdSHNL7wMMB0ZJGt5itx+Q5qreDjgc+B1ARNwQESMiYgRwFPB8RDxVqVhXpHmIDU8SZGb1ppIliJHAtIiYHhGLgJuAA1vsE8B62fNewEslzjMqO7bqPEmQmdWzzhU8dz9gZtHyLGDHFvucB/xL0qlAd+AzJc5zGB9OLABIOhE4EWDgwIFrGO6HvfACvP22G6jNrD7l3Ug9Crg6IvoD+wLXSfogJkk7Agsi4ulSB0fE5RHREBENffv2bfPgPAeEmdWzSiaI2cCAouX+2bpixwE3A0TEI0BXoE/R9sOBGysY4wo1J4itt84rAjOz/FQyQYwFhkjaTFIX0pf97S32eRHYE0DSMFKCmJstrwUcSk7tD5ASxODBsN56K93VzKzDqViCiIglwCnAncBkUm+liZJGSzog2+0M4ARJE0glhWMiIrJtuwEzI2J6pWJcGQ+xYWb1rJKN1ETEGGBMi3XnFD2fBOzSyrH3ATtVMr4Vef/9NA/EQQflFYGZWb7ybqRut6ZMgaVL3cXVzOqXE0QrPEmQmdU7J4hWFArQpQsMGZJ3JGZm+XCCaIUnCTKzeucE0QoPsWFm9c4JooQ33oDZs93+YGb1zQmiBA+xYWbmBFGSJwkyM3OCKKmxEXr3hk02yTsSM7P8OEGU4EmCzMycID6kqQmeftrtD2ZmThAtvPACvPOO2x/MzJwgWvAQG2ZmiRNEC809mLbaKt84zMzy5gTRQqEAm28OPXvmHYmZWb6cIFpo7sFkZlbvnCCKLFwIzzzjBGFmBhVOEJL2ljRV0jRJZ5XYPlDSvZLGS2qUtG/Rto9LekTSREkFSV0rGSvA5MlpkiAnCDOzCk45KqkTcAmwFzALGCvp9mya0WY/IM1Vfamk4aTpSQdL6gxcDxwVERMkfQRYXKlYm3mIDTOzZSpZghgJTIuI6RGxCLgJOLDFPgGslz3vBbyUPf8s0BgREwAi4vWIWFrBWIHUxXWddWCLLSp9JTOz9q+SCaIfMLNoeVa2rth5wJGSZpFKD6dm64cCIelOSU9K+k4F4/xAoQDDh0PnipWrzMxqR96N1KOAqyOiP7AvcJ2ktUhVX58Ejsj+PVjSni0PlnSipHGSxs2dO3eNg/EkQWZmy1QyQcwGBhQt98/WFTsOuBkgIh4BugJ9SKWNByLitYhYQCpdfKLlBSLi8ohoiIiGvn37rlGwr70Gc+a4gdrMrFklE8RYYIikzSR1AQ4Hbm+xz4vAngCShpESxFzgTmAbSd2yButPAZOoIE8SZGa2vIrVtkfEEkmnkL7sOwFXRsRESaOBcRFxO3AG8AdJp5MarI+JiADmSbqQlGQCGBMR/6xUrOAEYWbWUkWbYyNiDKl6qHjdOUXPJwG7tHLs9aSurlVRKECfPrDxxtW6oplZ+5Z3I3W70djoSYLMzIo5QZAmCZo40dVLZmbFnCCA55+Hd991F1czs2JOELiB2sysFCcIUvuD5EmCzMyKOUGwbJKg7t3zjsTMrP1wgsBDbJiZlVL3CeK99+DZZ93+YGbWUt0niLffhsMOg113zTsSM7P2pe4Htt5wQ/jTn/KOwsys/an7EoSZmZXmBGFmZiU5QZiZWUlOEGZmVpIThJmZleQEYWZmJTlBmJlZSU4QZmZWktIU0LVP0lzghVU4pA/wWoXCqUV+P5bn92MZvxfL62jvx6CI6FtqQ4dJEKtK0riIaMg7jvbC78fy/H4s4/diefX0friKyczMSnKCMDOzkuo5QVyedwDtjN+P5fn9WMbvxfLq5v2o2zYIMzNbsXouQZiZ2Qo4QZiZWUl1mSAk7S1pqqRpks7KO55qkjRA0r2SJkmaKOm0bH1vSXdJejb7d4O8Y60mSZ0kjZf0j2x5M0mPZZ+RP0vqkneM1SJpfUm3SJoiabKknev18yHp9Ozv5GlJN0rqWk+fjbpLEJI6AZcA+wDDgVGShucbVVUtAc6IiOHATsDXs9d/FnBPRAwB7smW68lpwOSi5Z8Dv4qILYB5wHG5RJWPi4A7IuJjwLak96XuPh+S+gHfABoiYmugE3A4dfTZqLsEAYwEpkXE9IhYBNwEHJhzTFUTEXMi4sns+dukP/5+pPfgmmy3a4CDcgkwB5L6A58HrsiWBewB3JLtUjfvh6RewG7AHwEiYlFEzKd+Px+dgXUldQa6AXOoo89GPSaIfsDMouVZ2bq6I2kwsB3wGLBRRMzJNr0MbJRXXDn4NfAdoClb/ggwPyKWZMv19BnZDJgLXJVVuV0hqTt1+PmIiNnABcCLpMTwJvAEdfTZqMcEYYCkHsBfgW9GxFvF2yL1fa6L/s+S9gNejYgn8o6lnegMfAK4NCK2A96lRXVSvXw+snaWA0lJc1OgO7B3rkFVWT0miNnAgKLl/tm6uiFpbVJyuCEibs1WvyJpk2z7JsCrecVXZbsAB0iaQapu3INUB79+Vq0A9fUZmQXMiojHsuVbSAmjHj8fnwGej4i5EbEYuJX0eambz0Y9JoixwJCsJ0IXUqPT7TnHVDVZ/fofgckRcWHRptuBo7PnRwN/r3ZseYiIsyOif0QMJn0W/h0RRwD3Al/Mdqun9+NlYKakLbNVewKTqM/Px4vATpK6ZX83ze9F3Xw26vJOakn7kuqdOwFXRsT5+UZUPZI+CTwIFFhW5/49UjvEzcBA0rDph0bEG7kEmRNJuwNnRsR+kjYnlSh6A+OBIyPi/RzDqxpJI0gN9l2A6cCxpB+Tdff5kPRfwGGk3n/jgeNJbQ518dmoywRhZmYrV49VTGZmVgYnCDMzK8kJwszMSnKCMDOzkpwgzMysJCcIs4ykh7N/B0v6chuf+3ulrmXWnrmbq1kLxfdDrMIxnYvG5ym1/Z2I6NEG4ZlVjUsQZhlJ72RPfwbsKumpbD6ATpJ+IWmspEZJJ2X77y7pQUm3k+6wRdJtkp7I5hA4MVv3M9KIoE9JuqH4Wkp+kc03UJB0WNG57yual+GG7G5eJP0sm8+jUdIF1XyPrL50XvkuZnXnLIpKENkX/ZsRsYOkdYCHJP0r2/cTwNYR8Xy2/NWIeEPSusBYSX+NiLMknRIRI0pc6xBgBGnehT7ZMQ9k27YDtgJeAh4CdpE0GTgY+FhEhKT12/almy3jEoTZyn0W+Iqkp0hDknwEGJJte7woOQB8Q9IE4FHSoJBDWLFPAjdGxNKIeAW4H9ih6NyzIqIJeAoYTBpyeiHwR0mHAAvW8LWZtcoJwmzlBJwaESOyx2YR0VyCePeDnVLbxWeAnSNiW9I4PV3X4LrF4/ssBZrbOUaSRlndD7hjDc5vtkJOEGYf9jbQs2j5TuDkbJh0JA3NJtFpqRcwLyIWSPoYaUrXZoubj2/hQeCwrJ2jL2k2t8dbCyybx6NXRIwBTidVTZlVhNsgzD6sEViaVRVdTZofYjDwZNZQPJfS00zeAXwtayeYSqpmanY50CjpyWw48WZ/A3YGJpAm4flORLycJZhSegJ/l9SVVLL51mq9QrMyuJurmZmV5ComMzMryQnCzMxKcoIwM7OSnCDMzKwkJwgzMyvJCcLMzEpygjAzs5L+H1vZWWHOSfU3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data['iteration'], data['accuracy'], color='b')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(\"Accuracy by Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba41782-fa0d-480e-bbd9-3b7c4f234b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "neural_network = NeuralNetwork(input_shape=input_shape, h1_shape=10, h2_shape=10, output_shape=output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03520403-de06-41c9-a417-dd339564c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training accuracy\n",
    "training_accuracy = neural_network.train(X_train, y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fdb259-76f4-4db3-84cd-fce0b3361149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training Accuracy: 0.9197'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Training Accuracy: {training_accuracy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e1af926-ff6d-4c06-bafc-db6e6e9a5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c94d837-3c58-4678-af9a-25b7e7afd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test accuracy\n",
    "test_accuracy = neural_network.accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db360984-4af8-463f-bbcd-998b4f4456c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test Accuracy: 0.9096'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Test Accuracy: {test_accuracy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bcda85c-1453-4417-b96e-6a73145ed846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34031a1b-9ab9-4af9-838d-43c93ae272dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.predict(X_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d552b-eff8-4086-8b62-f2dd0e13a2c9",
   "metadata": {},
   "source": [
    "# Conclusion and Further Topics of Neural Networks\n",
    "In this notebook I went over some basics of neural networks but there are somethings I wanted to mention as well. One of the things that we can implement in our neural network to give us better peformance is introduce a dropout layer. What a dropout layer is is a layer in which some number of neurons will turn to 0 no matter what. For example as we train our network and we are feeding forward an example we can choose to set 1 neuron in our layer to equal 0. This will make sure that the neural network doesn't just memorize the data and will prevent overfitting. It will make sure that it's actually learning the data and its important relationships. \n",
    "\n",
    "Another thing I wanted to go over was a different training method from the one used in this notebook. In this notebook we took the gradient of our cost function using our whole data. What we could do instead is do stocastic gradient descent which means instead of taking the gradient of our cost function for our whole data we take the gradient of our cost function using some mini batches. A mini batch is just our subset of our data. So instead of feeding all of our data for every iteration and performing back propagation we instead feed a mini batch of our data and calculate the gradient for that. Doing this will allow for faster training since we only have to take a smaller gradient for each mini batch.Neural networks are really powerful too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d3a66-854b-48d9-8ddc-e890cc6d14dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

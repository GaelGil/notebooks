{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274261cc-ff9f-4d9b-9f72-81ef0e65b5d7",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "In this notebook I will go over some of the basics of a fully connect feed forward neural network. I will mainly focus on their applications, how neural networks are formed, trained, and the math. To demonstrate this I will create my own neural network from scratch using `numpy` (a python library). \n",
    "\n",
    "First lets go over what data we are going to be using and what exactly we are trying to do. In this notebook I will use the use the mnist dataset (import from tensorflow) as an example. In this dataset we are given ,. [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) as an example. In this dataset we are given five columns `sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)` and `class`. The first four are all size measurements of a flowers sepal and petal. The `class` (label) column is used to represent if a flower is one of these three flowers `Iris-versicolor, Iris-setosa, Iris-virginica`. So now what we will attempt to do is classify each of these flowers given these measurements.\n",
    "\n",
    "Now that we know what this notebook is going go over I have to mention some things that will be helpful to know before reading. This notebook goes into the math of neural networks. Some math knowledge of matrix multiplication and multivariable calculus will be useful. We will be doing all the math with numpy so python is important as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26541d35-ee3e-404c-8101-42409b2771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0d811-41c6-4de2-b752-98f50ab9484d",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ef9278-e2e4-4b07-bce1-aac11da8e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daff26-8c49-4228-98d4-052d8fd10bdb",
   "metadata": {},
   "source": [
    "Taking a look at our data we can see that there are multiple variables. As I mentioned these variables are all size measurements of a flowers sepal and petal in cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbed065-7661-4726-ac66-76e677d503ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.title('A Digit in MNIST Dataset (label: 5)')\n",
    "plt.imshow(X_train[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadabf0a-130c-426a-a042-3439d1cebcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716d82a-6dfd-4f84-a542-72b169d77c84",
   "metadata": {},
   "source": [
    "<!-- Neural networks are very powerful tools that can help us solve problems with data that is non linear. For example if we have some data that is linear seperable we can easily draw a line to seperate and classify whatever it is we are trying to classify. But if we have some non linear data we can't do that. This is where neural networks come in. \n",
    "To further drive the point that simple logistic regression will not work we will graph the data to see what it looks like.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8363207e-935e-4a20-b773-ea2469c6e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, h1_shape, h2_shape, output_shape):\n",
    "        # layers/activations\n",
    "      \n",
    "        self.input = np.random.rand(input_shape, 1)- 0.5\n",
    "        self.h1 = np.random.rand(h1_shape, 1)- 0.5\n",
    "        self.h2 = np.random.rand(h2_shape, 1)- 0.5\n",
    "        self.h3 = np.random.rand(output_shape, 1)- 0.5\n",
    "        # weights \n",
    "        self.w_1 = np.random.rand(h1_shape, input_shape)- 0.5\n",
    "        self.w_2 = np.random.rand(h2_shape, h1_shape)-0.5\n",
    "        self.w_3 = np.random.rand(output_shape, h2_shape)- 0.5\n",
    "        \n",
    "        # biases \n",
    "        self.b_1 = np.random.rand(h1_shape, 1) - 0.5\n",
    "        self.b_2 = np.random.rand(h2_shape, 1) - 0.5\n",
    "        self.b_3 = np.random.rand(output_shape, 1) - 0.5 \n",
    "        \n",
    "    def relu(self,activations):\n",
    "        return np.maximum(0, activations)\n",
    "\n",
    "    def relu_deriv(self, activations):\n",
    "        return activations > 0\n",
    "\n",
    "    def softmax(self, activations):\n",
    "        return np.exp(activations) / np.sum(np.exp(activations))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # input\n",
    "        # reshapre the input into vector form.\n",
    "        # Example [1, 0, 0] -> [[1], [1], [1]]\n",
    "        self.input = np.reshape(X, (-1,1))\n",
    "        \n",
    "        # input -> h1\n",
    "        # the activations in the first hidden layer are given by the dot product \n",
    "        # of the weights by the input plus some biass its all then passed into\n",
    "        # our activation function. relu(W_1*x+b_1)\n",
    "        # TODO : fix feed forward (breaks at this line below)\n",
    "        h1_activations = self.relu(np.dot(self.w_1, self.input) + self.b_1)\n",
    "        self.h1 = h1_activations\n",
    "\n",
    "        # h1 -> h2\n",
    "        # the activations in the seocnd hidden layer (h_2) are given by the dot product \n",
    "        # of the second weights (w_2) by the previous activations (h1) plus the bias(b_2).\n",
    "        # W_2*h1+b_2\n",
    "        h2_activations = self.relu(np.dot(self.w_2, h1_activations) + self.b_2)\n",
    "        self.h2 = h2_activations \n",
    "        \n",
    "        h3_activations = np.dot(self.w_3, h2_activations) + self.b_3\n",
    "        self.h3 = h3_activations\n",
    "        # h2 -> output \n",
    "        # our output activtions/predictions are given by the second layer activations (h_2)\n",
    "        # put into the softmax function. \n",
    "        output = self.softmax(self.h3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def back_prop(self, output, y, learning_rate=0.01):\n",
    "        # print(output)\n",
    "        # print(y)\n",
    "        # y = np.reshape(y, (-1,1))\n",
    "        # error produced by the output\n",
    "        output_error = (2 * (output - y))\n",
    "        # print(output_error)\n",
    "\n",
    "        # derivative of cosf function with respect to third weights\n",
    "        dw_3 = -learning_rate * output_error.dot(self.h2.T)\n",
    "        # derivative of cost function with respect to the third biases\n",
    "        db_3 = -learning_rate * output_error\n",
    "        \n",
    "        # error produced by the second hidden layer activations\n",
    "        h2_error = self.w_3.T.dot(output_error) * self.relu_deriv(self.h2)\n",
    "        # derivative of cost function with respect to second weights\n",
    "        dw_2 = -learning_rate * h2_error.dot(self.h1.T)\n",
    "        # derivative of the cost function with respect to second biases\n",
    "        db_2 = -learning_rate * h2_error\n",
    "\n",
    "        # error produced by the first hidden layer activations\n",
    "        h1_error = self.w_2.T.dot(h2_error) * self.relu_deriv(self.h1)\n",
    "        dw_1 = -learning_rate * h1_error.dot(self.input.T)\n",
    "        db_1 = -learning_rate * h1_error\n",
    "        \n",
    "        # update all the weights\n",
    "        self.w_1 += dw_1\n",
    "        self.w_2 += dw_2\n",
    "        self.w_3 += dw_3\n",
    "        \n",
    "        # update all the biases\n",
    "        self.b_1 += db_1\n",
    "        self.b_2 += db_2\n",
    "        self.b_3 += db_3\n",
    "        return 0\n",
    "\n",
    "    def get_y_vector(self, y):\n",
    "        vector = [1 if i == y else 0 for i in range(len(self.h2))]\n",
    "        vector = np.reshape(vector, (-1, 1))\n",
    "        return vector\n",
    "       \n",
    "    def accuracy(self, X_data, y_data):\n",
    "        # TODO : X_data and y_data were previously wrong\n",
    "        total_correct = 0\n",
    "        for i in range(len(X_data)):\n",
    "            X = np.array(X_data[i].flatten())\n",
    "            y = self.get_y_vector(y_data[i])\n",
    "            # print(y)\n",
    "            output = self.feed_forward(X)\n",
    "            index_predicted = int(list(output).index(max(list(output))))\n",
    "            if list(y).index(max(y)) == index_predicted:\n",
    "                total_correct += 1\n",
    "        print(total_correct)\n",
    "        \n",
    "        return total_correct/len(X_data)\n",
    "    \n",
    "    def train(self, X_train, y_train, iterations):\n",
    "        for i in range(iterations):\n",
    "            for i in range(len(X_train)):\n",
    "                # X = X_train[i]/225\n",
    "                X = np.array(X_train[i].flatten())\n",
    "                # print(type(X))\n",
    "                y = self.get_y_vector(y_train[i])\n",
    "                output = self.feed_forward(X)\n",
    "                # print(y.shape)\n",
    "                # print(output.shape)\n",
    "                self.back_prop(output, y)\n",
    "        training_accuracy = self.accuracy(X_train, y_train)\n",
    "        return training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce48a95-b226-4103-b6a8-9e6ead4d5dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train[0].flatten().shape[0]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ee308b-156d-4cce-9545-f6aff018feaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = len(list(set(y_train)))\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a211dd2-3fef-48b3-b3c8-4412c110f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork(input_shape=input_shape, h1_shape=10, h2_shape=10, output_shape=output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553cac4a-bb48-4d1b-bb22-45c5d363d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca7cff5f-a674-42cd-a585-a96c85d946bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52359\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = neural_network.train(X_train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12c311f-7931-49a9-9caa-fbda515c1055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87265"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c94d837-3c58-4678-af9a-25b7e7afd30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52359\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = neural_network.accuracy(X_test/255, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db360984-4af8-463f-bbcd-998b4f4456c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2359"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b3c48b-0b53-404b-9aa0-3a084508678f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf32675-e250-484f-a020-db870aa7bcdc",
   "metadata": {},
   "source": [
    "# TODO: finish explaining the grpah\n",
    "Below is a scatter plot graph of what our data looks like. We have three different colors for representing each class. As we can see the data is not linearly seperable. We have three classes and they overlap. A model like linear regression will not be useful in helping us with classifying this. Also as we saw above our data is not two dimensional it is four dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d338f-e4ef-4496-9189-bccfae3738f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(setosa['sepal length (cm)'], setosa['sepal width (cm)'], color='b', label='setosa')\n",
    "plt.scatter(versicolor['sepal length (cm)'], versicolor['sepal width (cm)'], color='r', label='versicolor')\n",
    "plt.scatter(virginica['sepal length (cm)'], virginica['sepal width (cm)'], color='g', label='virginica')\n",
    "plt.legend(loc ='lower right')\n",
    "plt.xlabel('sepal length cm')\n",
    "plt.ylabel('sepal width cm')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f5d89-6b8e-4e88-a91d-bf30d8079819",
   "metadata": {},
   "source": [
    "Now that we have taken a look at our data we can get into creating a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6254fd-8c7c-48cf-8d3d-302e3e0a5049",
   "metadata": {},
   "source": [
    "# Neural Network Anatomy\n",
    "A neural network is formed by an input layer, some number of hidden layers, and an ouput layer. All of these layers contain some number of neurons. The input layer has as many neurons as it does inputs. For example in our network our input layer will have four neurons for `sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)`. Simiarly our output layer has as many neurons as it does classes. For example we have the three classes `Iris-versicolor, Iris-setosa, Iris-virginica` so we will have three output neurons. As I mentioned we can have as many hidden layers as we want. Within the hidden layers we can have as many neurons as we choose as well. To demonstrate what a nerual network looks like below is an image of the neural network that is made later in this notebook.\n",
    "# TODO add image of the neural network we are building\n",
    "\n",
    "The lines connecting the input layer to the first hidden layers are called weights same for the first hidden layer to the second hidden layers. These weights are what we will later be adjusting using back propogation.\n",
    "\n",
    "The idea of the nueral network is that we have some inputs and we want to pass them forward to through the network (from the input to hidden layers and to the output). The"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3cad9-74c3-495f-ba16-e363c8bd3eb6",
   "metadata": {},
   "source": [
    "# The Math (Feeding Forward)\n",
    "To feed forward an input from the input layer to the first hidden layer we will multiply all inputs $\\vec{x}$ by all the weights connecting the input layer to h1 $W_1$ and add some bias $b_1$. Doing all that will give us $h_1$. We then apply some non linearity function to that and that will be our activations. In this case we are using relu. This looks like this: \n",
    "$$ \\vec{z_1} = relu(\\vec{h_1})$$\n",
    "$$ \\vec{h_1} = W_1 * \\vec{x} + \\vec{b_1} $$\n",
    "$$ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$$\n",
    "\n",
    "Now we just continue this unitl we get to the output layer. So to keep forwarding this input we would now do this: \n",
    "\n",
    "$$ \\vec{z_2} = \\vec{h_2}$$\n",
    "$$ \\vec{h_2} = W_2 * \\vec{z_1} + \\vec{b_2} $$\n",
    "$$ \\vec{z_2} = W_2 * \\vec{z_1} + \\vec{b_2}$$\n",
    "\n",
    "And lastly our ouput will simply just be:\n",
    "$$ \\hat{y} = softmax(\\vec{z_2}) $$ \n",
    "Now that we know how to feed forward an input in a neural network we can go over how to train them. In the section below I will explain the back propagation. Back propagation is the algorithm that allows us to train our neural network so we can learn the correct value for our many parameters (weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9b866-5c46-44a6-a4ec-e0eee916eed6",
   "metadata": {},
   "source": [
    "# Back Propogation\n",
    "As I mentioned neural networks contain weights and biases. We can manually adjust them or set them to random variables to see which values give us the best result. This of course is not a good proccess seeing how our network has so many weights and biases. What we want to do instead is learn the values to our weights and biases that will minimise our loss. We can do this with calculus. We can do this by taking the negative gradient of our cost function ($ C = (\\hat{y}-y)^2$). The gradient gives the direction of fastest assent. So if we take the negative gradient we will approach a place in which our loss is as close to 0 as possible.  Below I will explain the math that allows us to learn the values for our weights and biases. This will involved mutlivariable calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009751c-da39-4b67-b374-e781e654b98d",
   "metadata": {},
   "source": [
    "This is a basic example of what our network looks like. We have an input($X$), two hidden layers ($h_1,h_2$) and two weights ($W_1,W_2$). \n",
    "$$ X ---W_1---> h_1 ---W_2---> h_2$$\n",
    "\n",
    "To calculate the error that our network produces we use the cost function $ C = (\\hat{y} - y)^2 $\n",
    "where $ \\hat{y} $ is the output vector/activations and $ y $ is the label as a vector. An example of what these look like is $ \\hat{y} = \\begin{bmatrix}\n",
    "0.8 \\\\\n",
    "0.1 \\\\\n",
    "0.1 \n",
    "\\end{bmatrix} $ and $ \\hat{y} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} $\n",
    "\n",
    "As I mentioned our output is $\\hat{y}$ which is given by the follwing function $\\hat{y} = softmax(h_2)$. The activations $h_2$ are passed trough the softmax function. \n",
    "\n",
    "$$ \\hat{y} = softmax( W_2 * \\vec{h_1} + \\vec{b_2} ) $$\n",
    "\n",
    "As we saw above to feed forward and input we use a series of functions. For example to get the first hidden layers activations we do this $ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$. What we want to do now is see how much each weight and bias is affecting our cost. In other words whats the derivative of the cost function $C$ with respect to $W_1$. \n",
    "\n",
    "In the example below we will take the derivative of our cost function with respect to the second weights ($W_2$).\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "\n",
    "$$ z^\\hat{y} = W_2 * \\vec{z_1} + \\vec{b_2}  \\hspace{10mm}  \\frac{\\partial z^\\hat{y}}{\\partial W_2} = \\vec{z_1}$$\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial z^\\hat{y}}{\\partial W_2} = 2(\\hat{y} - y) * \\vec{h_1} $$\n",
    "\n",
    "This will tell us by how much we should change the weights in $W_2$ Now we just continue this proccess until we reach the input layer. But first we need to calculate the error produced by the activations in the first hidden layer $\\frac{\\partial C}{\\partial h_1}$.\n",
    "\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "$$ \\hat{y} = h_2  \\hspace{10mm} \\frac{\\partial \\hat{y}}  {\\partial h_2}$$\n",
    "$$ h_2 = W_2 *z_1 + b_1 \\hspace{10mm} \\frac{\\partial h_2}  {\\partial z_1} = W_2$$\n",
    "$$ z_1 = relu(h_1) \\hspace{10mm} \\frac{\\partial z_1}  {\\partial h_1} = relu`(h_1)$$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial h_1} = \\frac{\\partial C}  {\\partial \\hat{y}} \\frac{\\partial \\hat{y}}  {\\partial h_2} \\frac{\\partial h_2}  {\\partial z_1} \\frac{\\partial z_1}  {\\partial h_1} = 2(\\hat{y} - y) *  W_2 * relu`(h_1)$$\n",
    "\n",
    "$$ 2(\\hat{y} - y) *  W_2 * relu`(h_1) $$ This will tell us the error in the first hidden layer. Now we can continue our back propogation. Our next derivative to get is going to be $\\frac{\\partial h_1}{\\partial W_1}$. This will tell use how to change the weights in $W_1$. Because we have calculated all of our chain rule deriatives all that is left is just now.\n",
    "\n",
    "$$ h_1 = \\vec{x}*W_1+b_1 \\hspace{10mm} \\frac{\\partial h_1}  {\\partial W_1} = \\vec{x}$$\n",
    "\n",
    "$$ \\frac{\\partial C}  {\\partial h_1} \\frac{\\partial h_1}  {\\partial W_1}  = \\vec{x}\\frac{\\partial C}  {\\partial h_1} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465205f8-0be9-4c4e-b096-11d97e29aae9",
   "metadata": {},
   "source": [
    "Now that we have talked about how a neural network feeds forward an input to get and output. And that we have went over how we can learn the values of the weights and biases of our neural network, it is time to build one. Below is the code to a neural network class which we will train to classify dataset. The network below has an input layer of four nerons for our for inputs, 2 hidden layers one of two neurons and one of 3 output neurons. I will go over how a neural network is trained below as well but as mentioned it involves the calculus that is explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e80ed6-9b67-4a33-baff-322cc4200ab2",
   "metadata": {},
   "source": [
    "# Creating the Network\n",
    "Below is the code for a neural network. As explained earlier our network will have a input layer, two hidden layers, and two weights. It has a function to feed forward an input and also to perform back propagation. Lastly it also has a function to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed078904-669e-48d0-a372-ae6da15c247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # layers/activations\n",
    "        self.input = np.random.rand(4, 1)\n",
    "        self.h1 = np.random.rand(2, 1)\n",
    "        self.h2 = np.random.rand(3, 1)\n",
    "        \n",
    "        # weights \n",
    "        self.w_1 = np.random.rand(2, 4)\n",
    "        self.w_2 = np.random.rand(3, 2)\n",
    "        \n",
    "        # biases \n",
    "        self.b_1 = np.random.rand(2, 1)\n",
    "        self.b_2 = np.random.rand(3, 1)\n",
    "\n",
    "    def relu(self,activations):\n",
    "        return np.maximum(0, activations)\n",
    "\n",
    "    def relu_deriv(self, activations):\n",
    "        return activations > 0\n",
    "\n",
    "    def softmax(self, activations):\n",
    "        return np.exp(activations) / np.sum(np.exp(activations))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # input\n",
    "        # reshapre the input into vector form.\n",
    "        # Example [1, 0, 0] -> [[1], [1], [1]]\n",
    "        self.input = np.reshape(X, (-1,1))\n",
    "        \n",
    "        # input -> h1\n",
    "        # the activations in the first hidden layer are given by the dot product \n",
    "        # of the weights by the input plus some biass its all then passed into\n",
    "        # our activation function. relu(W_1*x+b_1)\n",
    "        h1_activations = self.relu(np.dot(self.w_1, self.input) + self.b_1)\n",
    "        self.h1 = h1_activations\n",
    "\n",
    "        # h1 -> h2\n",
    "        # the activations in the seocnd hidden layer (h_2) are given by the dot product \n",
    "        # of the second weights (w_2) by the previous activations (h1) plus the bias(b_2).\n",
    "        # W_2*h1+b_2\n",
    "        h2_activations = (np.dot(self.w_2, h1_activations) + self.b_2)\n",
    "        self.h2 = h2_activations \n",
    "\n",
    "        # h2 -> output \n",
    "        # our output activtions/predictions are given by the second layer activations (h_2)\n",
    "        # put into the softmax function. \n",
    "        output = self.softmax(self.h2)\n",
    "        \n",
    "        return (output, max(output))\n",
    "    \n",
    "    def back_prop(self, output, y, learning_rate=0.01):\n",
    "        y = np.reshape(y, (-1,1))\n",
    "        \n",
    "        output_error = (2 * (output - y))\n",
    "        \n",
    "        dw_2 = -learning_rate * output_error.dot(self.h1.T)\n",
    "        b_2 = -learning_rate * output_error\n",
    "\n",
    "        h1_error = self.w_2.T.dot(output_error) * self.relu_deriv(self.h1)\n",
    "        dw_1 = -learning_rate * h1_error.dot(self.input.T)\n",
    "        b_1 = -learning_rate * h1_error\n",
    "        \n",
    "        # update all the weights\n",
    "        self.w_1 +=  dw_1\n",
    "        self.w_2 +=  dw_2\n",
    "        \n",
    "        # update all the biases\n",
    "        self.b_1 += b_1\n",
    "        self.b_2 += b_2\n",
    "        return 0\n",
    "       \n",
    "    def accuracy(self, data):\n",
    "        total_correct = 0\n",
    "        for index, row in data.iterrows():\n",
    "            # Our input value is the first four colmuns in our dataset.\n",
    "            # Since each row is turned into a list here we select the\n",
    "            # first four elements in the list whihch will be our intput.\n",
    "            X = row.tolist()[:4] \n",
    "            # Our output value is the last three colmuns in our dataset.\n",
    "            # To select it we select the last three elemets in the list\n",
    "            # that will be our output.\n",
    "            y = row.tolist()[4:]\n",
    "            output, predicted = self.feed_forward(X)\n",
    "            # get the index of the label (ie index = 2 [0, 0, 1])\n",
    "            label_index = y.index(max(y))\n",
    "            # check if the index of the label is the same as our\n",
    "            # index as the max value in our output vector\n",
    "            predicted_index = np.where(output==predicted)[0][0]\n",
    "            # if they are the same index increase the correct counter\n",
    "            if label_index == predicted_index:\n",
    "                total_correct += 1\n",
    "        return total_correct/data.shape[0]\n",
    "    \n",
    "    def train(self, data, iterations):\n",
    "        for i in range(iterations):\n",
    "            for index, row in data.iterrows():\n",
    "                # Our input value is the first four colmuns in our dataset.\n",
    "                # Since each row is turned into a list here we select the\n",
    "                # first four elements in the list whihch will be our intput.\n",
    "                X = row.tolist()[:4] \n",
    "                # Our output value is the last three colmuns in our dataset.\n",
    "                # To select it we select the last three elemets in the list\n",
    "                # that will be our output.\n",
    "                y = row.tolist()[4:]\n",
    "                output, predicted = self.feed_forward(X)\n",
    "                self.back_prop(output, y)\n",
    "        training_accuracy = self.accuracy(data)\n",
    "        return training_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39db34c-512b-4caa-aeed-2cebe509dc2d",
   "metadata": {},
   "source": [
    "# Using the Network\n",
    "Now that we have gone over the basics of neural networks we still have to do one more step. We want to find out how many iterations should we train our model for. For example should we train for `1` or `100`. We can do this by trying many iterations and seeing where the test loss decreases as our train loss increases. This will mean we are overfitting. Overfitting is when are just memorizing our data and not actually learning so our model only performs well on our training data but not test set. Lastly the loss is the error produced by the model for one example. Below I have a function to get the loss produced by each iteration so we can graph this out as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08957323-14f7-4c32-92c8-4567b14267a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [i for i in range(1, 1000, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c29037-4e36-4ea8-b7d8-43b205d4d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(iterations, train_data, test_data):\n",
    "    \"\"\"\n",
    "    function to get info on training data ie. loss and iterations\n",
    "    \"\"\"\n",
    "    training_info = {'iteration': [], 'training': [], 'test': []}\n",
    "    for i in range(len(iterations)):\n",
    "        nn = NeuralNetwork()\n",
    "        nn.train(train_data, iterations[i])\n",
    "        train_example = train_data.loc[np.random.choice(train_data.index, size=1)]\n",
    "        test_example = test_data.loc[np.random.choice(test_data.index, size=1)]\n",
    "        X_train = train_example.iloc[0].tolist()[:4] \n",
    "        y_train = train_example.iloc[0].tolist()[4:]\n",
    "        X_test = test_example.iloc[0].tolist()[:4]\n",
    "        y_test = test_example.iloc[0].tolist()[4:]\n",
    "        train_output, train_predicted = nn.feed_forward(X_train)\n",
    "        test_output, test_predicted = nn.feed_forward(X_test)\n",
    "        \n",
    "        train_loss = (1-train_predicted)\n",
    "        test_loss = (1-test_predicted)\n",
    "        \n",
    "        training_info['iteration'].append(iterations[i])\n",
    "        training_info['training'].append(train_loss)\n",
    "        training_info['test'].append(test_loss)\n",
    "    training_info = pd.DataFrame(data=training_info)\n",
    "    return training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799fcd8e-967f-498f-9721-16a0de7fe53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_info(iterations, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8816335-fb9e-4151-b25b-4f21108c9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7db159-cf10-4000-bab7-e26c16151fef",
   "metadata": {},
   "source": [
    "Here is our graph. #TODO : finish explaning this graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4db067-dde9-472f-877a-48db15584b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['iteration'], df['training'], color='b', label='training loss')\n",
    "plt.plot(df['iteration'], df['test'], color='r', label='test loss')\n",
    "plt.scatter(df['iteration'], df['training'], color='b')\n",
    "plt.scatter(df['iteration'], df['test'], color='r')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Loss by Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71f4d5-4105-4313-a5fe-24ffdcee7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc3e8c-79ed-4242-8d4d-8de4c7b0fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = neural_network.train(train_data, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260a947-45c6-4d63-a721-7783cb7908fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Accuracy: {training_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bdbbb-1015-41c2-8d54-44b18ea6ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test Accuracy: {neural_network.accuracy(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d552b-eff8-4086-8b62-f2dd0e13a2c9",
   "metadata": {},
   "source": [
    "# Further Topics of Neural Networks\n",
    "In this notebook I went over some basics of neural networks but there are somethings I wanted to mention as well. One of the things that we can implement in our neural network to give us better peformance is introduce a dropout layer. What a dropout layer is is a layer in which some number of neurons will turn to 0 no matter what. For example as we train our network and we are feeding forward an example we can choose to set 1 neuron in our layer to equal 0. This will make sure that the neural network doesn't just memorize the data and overfit. It will make sure that it's actually learning the data and its important relationships. \n",
    "\n",
    "Another thing I wanted to go over was a different training method from the one used in this notebook. In this notebook we took the gradient of our cost function using our whole data. What we could do instead is do stocastic gradient descent which means instead of taking the gradient of our cost function for our whole data we take the gradient of our cost function using some mini batches. A mini batch is just our sbuset our data. So instead of feeding all of our data for every iteration and performing back propagation we instead feed a mini batch of our data and calculate the gradient for that. Doing this will allow for faster training since we only have to take a smaller gradient for each mini batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3055ed1-11ee-4427-8302-4d217bea94a6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4576778-e372-4fba-8ee6-6a715519f2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

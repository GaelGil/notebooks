{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274261cc-ff9f-4d9b-9f72-81ef0e65b5d7",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "In this notebook I will go over some of the basics of a fully connect feed forward neural network. I will mainly focus on applyling one to a problem, how neural networks are formed, trained, and the math behind them. To demonstrate this I will create my own neural network from scratch using `numpy` (a python library). In this notebook I will be using the mnist dataset (imported from tensorflow). In this dataset we are given a image a number (0-9) and its label (what number it is). I will go into more detail about the data later in the notebook. What we will do in this noktebook is create a neural network that takes a image of a hand written digit and classifies that image correctly. \n",
    " \n",
    "Now that we know what this notebook is goint to cover I have to mention some things that will be helpful to know before reading. This notebook goes into the math of neural networks. Some math knowledge of matrix multiplication and multivariable calculus will be useful. We will be doing all the math with numpy so python is important as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26541d35-ee3e-404c-8101-42409b2771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0d811-41c6-4de2-b752-98f50ab9484d",
   "metadata": {},
   "source": [
    "# The Data\n",
    "As I mentioned the data is the mnist dataset that we imported from tesorflow. We are given an image of a hand written digit (0-9) and its label. These are our X (digit) and y (label). The image of the digit is represented by a 28 by 28 matrix where each value of the matrix is a pixel in the image. The label is just simply a nuber. Below I have imported the data and split it into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ef9278-e2e4-4b07-bce1-aac11da8e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daff26-8c49-4228-98d4-052d8fd10bdb",
   "metadata": {},
   "source": [
    "Below is an image of a digit in our dataset. We can clearly see that it is a number five. As I mentioned this is simply just a 28 by 28 matrix where each value in the pixel. So the black parts are a value in the matrix (pixel) that is probably 0. However the actual number 5 probably has higher values like 200. The gray parts are somewhere in between those two values. One of the reasons for why we are using a neural network in to classify these digits is because... This dataset is very high in dimensions. That means that our X (input) has a lot of dimensions. Each input has a size of `28*28=784`. We cannot graph this dataset without using a dimensionality algoritm like PCA or TSNE to reduce the dimensions so we can graph it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbed065-7661-4726-ac66-76e677d503ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.title('A Digit in MNIST Dataset (label: 5)')\n",
    "plt.imshow(X_train[0]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5312eac5-221d-49ac-b836-1c7d0fab08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0] # label of the digit above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2f9e92-1d98-4737-986f-768a7be46c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d460c1c-8491-4260-b06f-0422016ff418",
   "metadata": {},
   "source": [
    "Now that we have taken a look at our data we can get into creating a neural network.\n",
    "\n",
    "# Neural Network Anatomy\n",
    "A neural network is formed by an input layer, some number of hidden layers, and an ouput layer. All of these layers contain some number of neurons. The input layer has as many neurons as it does inputs. For example in our network our input layer will have 784 neuros one for each pixel in the image. Similarly our output will layer will have ten neurons one for each number 1 through 9. In terms of hidden layers we can have as many as we choose but we also wan't to make sure we have a correct ammount. If we have too many or too little it could affect the performance of the neural network (same goes for neurons in the hidden layers). In the neural network built later in this notebook I choose to have two hidden layesr with 10 neurons. This might be a lot to just think about so below is an image of what the neural network we are building later will look like. \n",
    "\n",
    "# TODO add image of the neural network we are building\n",
    "The lines that are connecting the layers in the image above are called weights. These weights are what we are trying to learn this value of. We want to adjust them carefully and into a direction that gives us the lowest error when training. W can choose to set them randomly or to chnage them manually but that is not an efficient system. But now that we know what makes up a neural network and now that we know that we are trying to learn the values of these weights how do we even use them?\n",
    "\n",
    "To answer that question I will first go over feeding forward. Below I will explain how it is done. \n",
    "\n",
    "# The Math (Feeding Forward)\n",
    "Below I will go over feeding forward in a nueral network. Feeding forward is when we pass a input through the neural network to get an output. I will use the example of how the neural network we are buidling later will feed forward an input to get an output. \n",
    "\n",
    "First to feed foward the input layer to the first hidden layer we will multiply all inputs $\\vec{x}$ by all the weights connecting the input layer to h1 $W_1$ and add some bias $b_1$. Doing all that will give us $h_1$. We then apply some non linearity function to that and that will be our activations. In this case we are using relu. This looks like this:\n",
    "\n",
    "$$ \\vec{h_1} = relu(\\vec{z_1})$$\n",
    "$$ \\vec{z_1} = W_1 * \\vec{x} + \\vec{b_1} $$\n",
    "$$ \\vec{h_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$$\n",
    "\n",
    "Now we just continue this proccess unitl we get to the output layer. So to keep forwarding this input we would now do this: \n",
    "\n",
    "$$ \\vec{h_2} = relu(\\vec{z_2})$$\n",
    "$$ \\vec{h_2} = W_2 * \\vec{z_1} + \\vec{b_2} $$\n",
    "$$ \\vec{h_2} = relu(W_2 * \\vec{z_1} + \\vec{b_2})$$\n",
    "\n",
    "Next we will do:\n",
    "$$ \\vec{h_3} = \\vec{z_3}$$\n",
    "$$ \\vec{z_3} = W_3 * \\vec{z_2} + \\vec{b_3} $$\n",
    "$$ \\hat{y} = \\vec{h_3}$$\n",
    "\n",
    "\n",
    "And lastly our ouput will simply just be:\n",
    "$$ \\hat{y} = softmax(\\vec{h_3}) $$ \n",
    "Softmax will turn all of the probabilities in the output layer to be values between 0 and 1. Now that we know how the feed forward function works we need to understand how to get the correct output we want when we pass in a input. We do this using back propagation which I will explain in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215472be-c1e3-4a5e-8b66-779f95dc32d4",
   "metadata": {},
   "source": [
    "# Back Propogation TODO: finish explaining back propagation\n",
    "The previous section went over how feed forward works in a neural network to get an output so we can classify (a image of a digit for example) something. \n",
    "As I mentioned neural networks contain weights and biases. We can manually adjust them or set them to random variables to see which values give us the best result. This of course is not a good proccess seeing how our network has so many weights and biases. What we want to do instead is learn the values to our weights and biases that will minimise our loss. We can do this with calculus. We can do this by taking the negative gradient of our cost function ($ C = (\\hat{y}-y)^2$). The gradient gives the direction of fastest assent. So if we take the negative gradient we will approach a place in which our loss is as close to 0 as possible.  Below I will explain the math that allows us to learn the values for our weights and biases. This will involved mutlivariable calculus.\n",
    "\n",
    "\n",
    "This is a basic example of what our network looks like. We have an input($X$), two hidden layers ($h_1,h_2$) and two weights ($W_1,W_2$). \n",
    "$$ X ---W_1---> h_1 ---W_2---> h_2$$\n",
    "\n",
    "To calculate the error that our network produces we use the cost function $ C = (\\hat{y} - y)^2 $\n",
    "where $ \\hat{y} $ is the output vector/activations and $ y $ is the label as a vector. An example of what these look like is $ \\hat{y} = \\begin{bmatrix}\n",
    "0.8 \\\\\n",
    "0.1 \\\\\n",
    "0.1 \n",
    "\\end{bmatrix} $ and $ \\hat{y} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} $\n",
    "\n",
    "As I mentioned our output is $\\hat{y}$ which is given by the follwing function $\\hat{y} = softmax(h_2)$. The activations $h_2$ are passed trough the softmax function. \n",
    "\n",
    "$$ \\hat{y} = softmax( W_2 * \\vec{h_1} + \\vec{b_2} ) $$\n",
    "\n",
    "As we saw above to feed forward and input we use a series of functions. For example to get the first hidden layers activations we do this $ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$. What we want to do now is see how much each weight and bias is affecting our cost. In other words whats the derivative of the cost function $C$ with respect to $W_1$. \n",
    "\n",
    "In the example below we will take the derivative of our cost function with respect to the second weights ($W_2$).\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "\n",
    "$$ z^\\hat{y} = W_2 * \\vec{z_1} + \\vec{b_2}  \\hspace{10mm}  \\frac{\\partial z^\\hat{y}}{\\partial W_2} = \\vec{z_1}$$\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial z^\\hat{y}}{\\partial W_2} = 2(\\hat{y} - y) * \\vec{h_1} $$\n",
    "\n",
    "This will tell us by how much we should change the weights in $W_2$ Now we just continue this proccess until we reach the input layer. But first we need to calculate the error produced by the activations in the first hidden layer $\\frac{\\partial C}{\\partial h_1}$.\n",
    "\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "$$ \\hat{y} = h_2  \\hspace{10mm} \\frac{\\partial \\hat{y}}  {\\partial h_2}$$\n",
    "$$ h_2 = W_2 *z_1 + b_1 \\hspace{10mm} \\frac{\\partial h_2}  {\\partial z_1} = W_2$$\n",
    "$$ z_1 = relu(h_1) \\hspace{10mm} \\frac{\\partial z_1}  {\\partial h_1} = relu`(h_1)$$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial h_1} = \\frac{\\partial C}  {\\partial \\hat{y}} \\frac{\\partial \\hat{y}}  {\\partial h_2} \\frac{\\partial h_2}  {\\partial z_1} \\frac{\\partial z_1}  {\\partial h_1} = 2(\\hat{y} - y) *  W_2 * relu`(h_1)$$\n",
    "\n",
    "$$ 2(\\hat{y} - y) *  W_2 * relu`(h_1) $$ This will tell us the error in the first hidden layer. Now we can continue our back propogation. Our next derivative to get is going to be $\\frac{\\partial h_1}{\\partial W_1}$. This will tell use how to change the weights in $W_1$. Because we have calculated all of our chain rule deriatives all that is left is just now.\n",
    "\n",
    "$$ h_1 = \\vec{x}*W_1+b_1 \\hspace{10mm} \\frac{\\partial h_1}  {\\partial W_1} = \\vec{x}$$\n",
    "\n",
    "$$ \\frac{\\partial C}  {\\partial h_1} \\frac{\\partial h_1}  {\\partial W_1}  = \\vec{x}\\frac{\\partial C}  {\\partial h_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e28e26-ec59-4abd-946e-cb3863fe764a",
   "metadata": {},
   "source": [
    "Now that we have talked about how a neural network feeds forward an input to get and output. And that we have went over how we can learn the values of the weights and biases of our neural network, it is time to build one. Below is the code to a neural network class which we will train to classify dataset. The network below has an input layer of four nerons for our for inputs, 2 hidden layers one of two neurons and one of 3 output neurons. I will go over how a neural network is trained below as well but as mentioned it involves the calculus that is explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c4065e-a752-45ac-8563-97770ba9bd97",
   "metadata": {},
   "source": [
    "# Creating the Network\n",
    "Now that we know how we feed forward a input to get a output and how we learn the correct values for the weights in our neural network we can start to implement it. Below is a class representing a nueral network. As explained earlier the network will have a input layer, three hidden layers and three weights and biases. The feed forward function will do the exact same thing that was explained above. Same for the back propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8363207e-935e-4a20-b773-ea2469c6e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_shape, h1_shape, h2_shape, output_shape):\n",
    "        # layers/activations\n",
    "        self.input = np.random.rand(input_shape, 1)- 0.5\n",
    "        self.h1 = np.random.rand(h1_shape, 1)- 0.5\n",
    "        self.h2 = np.random.rand(h2_shape, 1)- 0.5\n",
    "        self.h3 = np.random.rand(output_shape, 1)- 0.5\n",
    "        # weights \n",
    "        self.w_1 = np.random.rand(h1_shape, input_shape)- 0.5\n",
    "        self.w_2 = np.random.rand(h2_shape, h1_shape)-0.5\n",
    "        self.w_3 = np.random.rand(output_shape, h2_shape)- 0.5\n",
    "        \n",
    "        # biases \n",
    "        self.b_1 = np.random.rand(h1_shape, 1) - 0.5\n",
    "        self.b_2 = np.random.rand(h2_shape, 1) - 0.5\n",
    "        self.b_3 = np.random.rand(output_shape, 1) - 0.5 \n",
    "        \n",
    "    def relu(self,activations):\n",
    "        return np.maximum(0, activations)\n",
    "\n",
    "    def relu_deriv(self, activations):\n",
    "        return activations > 0\n",
    "\n",
    "    def softmax(self, activations):\n",
    "        return np.exp(activations) / np.sum(np.exp(activations))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # input\n",
    "        # reshapre the input into vector form.\n",
    "        # Example [1, 0, 0] -> [[1], [1], [1]]\n",
    "        self.input = np.reshape(X, (-1,1))\n",
    "        \n",
    "        # input -> h1\n",
    "        # the activations in the first hidden layer are given by the dot product \n",
    "        # of the weights by the input plus some biass its all then passed into\n",
    "        # our activation function. relu(W_1*x+b_1)\n",
    "        z1 = self.relu(np.dot(self.w_1, self.input) + self.b_1)\n",
    "        self.h1 = z1\n",
    "        \n",
    "        # h1 -> h2\n",
    "        # the activations in the seocnd hidden layer (h_2) are given by the dot product \n",
    "        # of the second weights (w_2) by the previous activations (h1) plus the bias(b_2).\n",
    "        # W_2*h1+b_2\n",
    "        z2 =  self.relu(np.dot(self.w_2, z1) + self.b_2)\n",
    "        self.h2 = z2 \n",
    "        \n",
    "        \n",
    "        z3 = np.dot(self.w_3, z2) + self.b_3\n",
    "        self.h3 = z3\n",
    "        # h2 -> output \n",
    "        # our output activtions/predictions are given by the second layer activations (h_2)\n",
    "        # put into the softmax function. \n",
    "        output = self.softmax(self.h3)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def back_prop(self, output, y, learning_rate=0.01):\n",
    "        # error produced by the output\n",
    "        output_error = (2 * (output - y))\n",
    "\n",
    "        # derivative of cosf function with respect to third weights\n",
    "        dw_3 = -learning_rate * output_error.dot(self.h2.T)\n",
    "        # derivative of cost function with respect to the third biases\n",
    "        db_3 = -learning_rate * output_error\n",
    "        \n",
    "        # error produced by the second hidden layer activations\n",
    "        h2_error = self.w_3.T.dot(output_error) * self.relu_deriv(self.h2)\n",
    "        # derivative of cost function with respect to second weights\n",
    "        dw_2 = -learning_rate * h2_error.dot(self.h1.T)\n",
    "        # derivative of the cost function with respect to second biases\n",
    "        db_2 = -learning_rate * h2_error\n",
    "\n",
    "        # error produced by the first hidden layer activations\n",
    "        h1_error = self.w_2.T.dot(h2_error) * self.relu_deriv(self.h1)\n",
    "        dw_1 = -learning_rate * h1_error.dot(self.input.T)\n",
    "        db_1 = -learning_rate * h1_error\n",
    "        \n",
    "        # update all the weights\n",
    "        self.w_1 += dw_1\n",
    "        self.w_2 += dw_2\n",
    "        self.w_3 += dw_3\n",
    "        \n",
    "        # update all the biases\n",
    "        self.b_1 += db_1\n",
    "        self.b_2 += db_2\n",
    "        self.b_3 += db_3\n",
    "        return 0\n",
    "\n",
    "    def get_y_vector(self, y):\n",
    "        vector = [1 if i == y else 0 for i in range(len(self.h2))]\n",
    "        vector = np.reshape(vector, (-1, 1))\n",
    "        return vector\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = self.feed_forward(np.array(X.flatten()))\n",
    "        return np.argmax(prediction)\n",
    "\n",
    "    def get_prediction(self, output_layer):\n",
    "        return np.argmax(output_layer)\n",
    "       \n",
    "    def accuracy(self, X_data, y_data):\n",
    "        correct = 0\n",
    "        for i in range(len(X_data)):\n",
    "            X = np.array(X_data[i].flatten())\n",
    "            y = self.get_y_vector(y_data[i])\n",
    "            output = self.feed_forward(X)  \n",
    "            prediction = self.get_prediction(output)\n",
    "            if np.argmax(y) == prediction:\n",
    "                correct += 1                \n",
    "        return correct/X_data.shape[0]\n",
    "    \n",
    "    def train(self, X_data, y_data, iterations):\n",
    "        for i in range(iterations):\n",
    "            for i in range(len(X_data)):\n",
    "                X = np.array(X_data[i].flatten())\n",
    "                y = self.get_y_vector(y_data[i])\n",
    "                output = self.feed_forward(X)\n",
    "                self.back_prop(output, y)\n",
    "        training_accuracy = self.accuracy(X_data, y_data)\n",
    "        return training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce48a95-b226-4103-b6a8-9e6ead4d5dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train[0].flatten().shape[0]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ee308b-156d-4cce-9545-f6aff018feaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = len(list(set(y_train)))\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a211dd2-3fef-48b3-b3c8-4412c110f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = NeuralNetwork(input_shape=input_shape, h1_shape=10, h2_shape=10, output_shape=output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "553cac4a-bb48-4d1b-bb22-45c5d363d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca7cff5f-a674-42cd-a585-a96c85d946bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = neural_network.train(X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2fdb259-76f4-4db3-84cd-fce0b3361149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9195833333333333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1393e6c5-3f3e-4e3e-aa28-23d4ebebc38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(neural_network.h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e1af926-ff6d-4c06-bafc-db6e6e9a5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c94d837-3c58-4678-af9a-25b7e7afd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = neural_network.accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db360984-4af8-463f-bbcd-998b4f4456c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9092"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bcda85c-1453-4417-b96e-6a73145ed846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34031a1b-9ab9-4af9-838d-43c93ae272dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.predict(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b19cbc5-98de-4a71-b592-44548c3c8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(train_X, train_y, test_X, test_y, iterations):\n",
    "    training_info = {'iteration': [], 'training': [], 'test': []}\n",
    "    for i in range(len(iterations)):\n",
    "        nn = NeuralNetwork(input_shape=input_shape, h1_shape=10, h2_shape=10, output_shape=output_shape)\n",
    "        training_accuracy = neural_network.train(train_X, train_y, iterations[i])\n",
    "\n",
    "        train_example = train_X[np.random.randint(low=0, high=train_X.shape[0])]\n",
    "        test_example = test_X[np.random.randint(low=0, high=test_X.shape[0])]\n",
    "        \n",
    "        train_output = nn.feed_forward(train_example)\n",
    "        test_output = nn.feed_forward(test_example)\n",
    "        \n",
    "        train_loss = ((np.argmax(train_output)-1)**2)/train_X.shape[0]\n",
    "        test_loss = ((np.argmax(test_output)-1)**2)/train_X.shape[0]\n",
    "        \n",
    "        training_info['iteration'].append(iterations[i])\n",
    "        training_info['training'].append(train_loss)\n",
    "        training_info['test'].append(test_loss)\n",
    "    training_info = pd.DataFrame(data=training_info)\n",
    "    return training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94e72cb8-3f82-4a0e-baae-dcb07e432383",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [i for i in range(1, 50, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c9926e1-69f8-4c79-bfb5-c93bb44104c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_info(X_train, y_train, X_test, y_train, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d63674f-966c-4ccf-93d3-f6393161ad65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(data['iteration'], data['training'], color='b', label='training loss')\n",
    "# plt.plot(data['iteration'], data['test'], color='r', label='test loss')\n",
    "# plt.scatter(data['iteration'], data['training'], color='b')\n",
    "plt.scatter(data['iteration'], data['test'], color='r')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Loss by Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d552b-eff8-4086-8b62-f2dd0e13a2c9",
   "metadata": {},
   "source": [
    "# Further Topics of Neural Networks\n",
    "In this notebook I went over some basics of neural networks but there are somethings I wanted to mention as well. One of the things that we can implement in our neural network to give us better peformance is introduce a dropout layer. What a dropout layer is is a layer in which some number of neurons will turn to 0 no matter what. For example as we train our network and we are feeding forward an example we can choose to set 1 neuron in our layer to equal 0. This will make sure that the neural network doesn't just memorize the data and will prevent overfitting. It will make sure that it's actually learning the data and its important relationships. \n",
    "\n",
    "Another thing I wanted to go over was a different training method from the one used in this notebook. In this notebook we took the gradient of our cost function using our whole data. What we could do instead is do stocastic gradient descent which means instead of taking the gradient of our cost function for our whole data we take the gradient of our cost function using some mini batches. A mini batch is just our subset of our data. So instead of feeding all of our data for every iteration and performing back propagation we instead feed a mini batch of our data and calculate the gradient for that. Doing this will allow for faster training since we only have to take a smaller gradient for each mini batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3055ed1-11ee-4427-8302-4d217bea94a6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4576778-e372-4fba-8ee6-6a715519f2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b188161c-acbc-4b0e-b21c-16e5d2209464",
   "metadata": {},
   "source": [
    "# Back Propagations/Neural NetworkNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa910f74-007a-439e-a24c-d77ff392bd90",
   "metadata": {},
   "source": [
    "Neural networks have a input layer. The input layer is of course the layer that takes in the inputs. We have as many input neurons as we do inputs. For example if we have a dataset of cats and dogs some example inputs would be `ear length`, `fur length`, `weight`, `height`. This would mean we would have a input layer with 4 neurons. These inputs would be in the input layer and would then be passed forward to the next layer. We pass them by getting the sum of the weights times the inputs and adding a bias(sum number). That value is then passed into an activation function. A common function is the sigmoid function so our values will be between 0 and 1. Depening on the value we get from the activation function the neuron will feed forward that information to the next layer or it will not (depends on value). \n",
    "<!-- Each nueron has a bias which is a number between 0 and 1. -->\n",
    "Depending on how many hidden layers we have this would happen again passing from one layer to another. But eventually we would get to the output layer which in this case we have a output layer for 2 neurons. One for dog and one for cat. These would output the probability from 1 to 0 of it being a dog or cat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77061bfa-e585-47ba-ae8f-167f35790d5d",
   "metadata": {},
   "source": [
    "The math the sum of the weights times the neurosn plus the bias looks like this: Where $w$ is a weight, $n$ is a neuron and $b$ is the bias of the neuron we are moving the data forward to\n",
    "$$ = (w_0*n_0 + w_1*n_1 + w_2*n_2 + w_3*n_3 .... + w_n*n_n)+b $$\n",
    "we can also write this using vectors: \n",
    "\n",
    "$$\\vec{w} =\n",
    "\\begin{bmatrix}\n",
    "w_{0,0} & w_{0,1} & w_{0,3}\\\\\n",
    " & .... &  \\\\\n",
    "  &   w_{n,n} & \n",
    "\\end{bmatrix} * \\vec{n} = [n_0, n_1, n_2, n_3 ..... i_n] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779b5f3-3d77-4852-876b-b7e008c715e1",
   "metadata": {},
   "source": [
    "This is of course how the neural network works but there is a lot going on. How is trained, how do we determine the weight and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa9f0e-2881-4e9b-b0c7-c2d0fddc51dd",
   "metadata": {},
   "source": [
    "How does the cost function change with respect to weights, bias, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206560af-37ef-4f23-bce7-bee0988fc777",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eac3c19-e91e-4350-bc6f-12031ef74c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b188161c-acbc-4b0e-b21c-16e5d2209464",
   "metadata": {},
   "source": [
    "# Back Propagations/Neural NetworkNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa910f74-007a-439e-a24c-d77ff392bd90",
   "metadata": {},
   "source": [
    "Neural networks have a input layer. The input layer is of course the layer that takes in the inputs. We have as many input neurons as we do inputs. For example if we have a dataset of cats and dogs some example inputs would be `ear length`, `fur length`, `weight`, `height`. This would mean we would have a input layer with 4 neurons. These inputs would be in the input layer and would then be passed forward to the next layer. We pass them by getting the sum of the weights times the inputs and adding a bias(sum number). That value is then passed into an activation function. A common function is the sigmoid function so our values will be between 0 and 1. Depening on the value we get from the activation function the neuron will feed forward that information to the next layer or it will not (depends on value). \n",
    "<!-- Each nueron has a bias which is a number between 0 and 1. -->\n",
    "Depending on how many hidden layers we have this would happen again passing from one layer to another. But eventually we would get to the output layer which in this case we have a output layer for 2 neurons. One for dog and one for cat. These would output the probability from 1 to 0 of it being a dog or cat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77061bfa-e585-47ba-ae8f-167f35790d5d",
   "metadata": {},
   "source": [
    "The math the sum of the weights times the neurosn plus the bias looks like this: Where $w$ is a weight, $n$ is a neuron and $b$ is the bias of the neuron we are moving the data forward to\n",
    "$$ = (w_0*n_0 + w_1*n_1 + w_2*n_2 + w_3*n_3 .... + w_n*n_n)+b $$\n",
    "we can also write this using vectors: \n",
    "\n",
    "$$\\vec{w} =\n",
    "\\begin{bmatrix}\n",
    "w_{0,0} & w_{0,1} & w_{0,3}\\\\\n",
    " & .... &  \\\\\n",
    "  &   w_{n,n} & \n",
    "\\end{bmatrix} * \\vec{n} = [n_0, n_1, n_2, n_3 ..... i_n] + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8bfd5598-27d2-4415-9c98-2545ea17a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap # for grgphing decision boundaries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da38e9f5-61b6-4255-a0d2-e846df2e6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "X_train = pd.read_csv(f'./{data_folder}/X_train.csv')\n",
    "y_train = pd.read_csv(f'./{data_folder}/y_train.csv')\n",
    "X_test = pd.read_csv(f'./{data_folder}/X_test.csv')\n",
    "y_test = pd.read_csv(f'./{data_folder}/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb133230-6cca-45f3-9661-a96038245748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.5               2.4                3.7               1.0\n",
       "1                  4.8               3.0                1.4               0.1\n",
       "2                  5.5               2.6                4.4               1.2\n",
       "3                  5.0               3.2                1.2               0.2\n",
       "4                  6.9               3.1                5.1               2.3\n",
       "..                 ...               ...                ...               ...\n",
       "107                6.3               2.7                4.9               1.8\n",
       "108                7.2               3.0                5.8               1.6\n",
       "109                5.8               4.0                1.2               0.2\n",
       "110                5.2               3.4                1.4               0.2\n",
       "111                7.2               3.2                6.0               1.8\n",
       "\n",
       "[112 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1453427-f21c-4424-b311-edc61a32a14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[sepal length (cm)    5.5\n",
       " sepal width (cm)     2.4\n",
       " petal length (cm)    3.7\n",
       " petal width (cm)     1.0\n",
       " Name: 0, dtype: float64]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_train[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].iloc[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e33661-9370-4c0e-b5f7-072ba9008132",
   "metadata": {},
   "source": [
    "# Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6529eae0-ab0a-4575-8c0d-bf3066532acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of input neurons: 4'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the inputs\n",
    "inputs = [X_train.iloc[0]['sepal length (cm)'], X_train.iloc[0]['sepal width (cm)'], X_train.iloc[0]['petal length (cm)'], X_train.iloc[0]['petal length (cm)']]\n",
    "f'Number of input neurons: {len(inputs)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a23c8e16-6d0f-4f0d-b401-50052c274e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of neurons at second layer: 4'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a second layer\n",
    "second_layer = [random.uniform(0, 1)] * 4\n",
    "f'Number of neurons at second layer: {len(second_layer)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ccf32-d3a0-4cad-a81c-900efb1b8519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9f63c627-fffe-4fc7-944c-8182de43251e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of weights from input layer to second: 16'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The weights connecting the input layer to second layer\n",
    "w, h = len(inputs), len(second_layer)\n",
    "input_to_second_layer_weights = [[random.uniform(0, 1) for x in range(w)] for y in range(h)]\n",
    "input_to_second_layer_weights = np.array(input_to_second_layer_weights)\n",
    "input_to_second_layer_weights\n",
    "f'Number of weights from input layer to second: {(input_to_second_layer_weights.size)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5b5002fc-7379-4dc2-a69b-4b0ce68fe736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of neurons at third layer: 3'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a third layer\n",
    "third_layer = [random.uniform(0, 1)] * 3\n",
    "f'Number of neurons at third layer: {len(third_layer)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2ba9942e-2fc7-4c85-9743-bc2877f07461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of weights from second layer to third: 12'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_second_to_third_layer_weights = (len(second_layer)*(len(third_layer)))\n",
    "# The weights connecting the second layer to third layer\n",
    "second_to_third_layer_weights = [random.uniform(0, 1)] * num_second_to_third_layer_weights\n",
    "f'Number of weights from second layer to third: {len(second_to_third_layer_weights)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "012e5eb1-ca0f-4803-82b4-0d734d64bc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of neurons at output layer: 3'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the output values to get number of outpts\n",
    "output_layer_length = len(set((y_train.iloc[:, 0].tolist())))\n",
    "output_layer = [random.uniform(0, 1)] * output_layer_length\n",
    "f'Number of neurons at output layer: {len(output_layer)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e08ee12-858f-4745-807c-07b38ec49b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of weights from third layer to output: 9'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_third_to_output_layer_weights = (len(third_layer)*(len(output_layer)))\n",
    "# The weights connecting the third layer to output layer\n",
    "third_to_output_layer_weights = [random.uniform(0, 1)] * num_third_to_output_layer_weights\n",
    "f'Number of weights from third layer to output: {len(third_to_output_layer_weights)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e3788-d191-4faa-9db3-2b162b01e0c4",
   "metadata": {},
   "source": [
    "# Feeding forward the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "55d7c3e0-3f29-4a5e-98dd-4687791fc3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(num):\n",
    "    return (1/(1+np.exp(-(num))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0f794e09-64e8-42fa-bb3a-cefdbf2c673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(weights, layer):\n",
    "    for w in weights:\n",
    "        activation = sigmoid(np.dot(w, layer))\n",
    "        print(activation)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a9e41573-33bf-4015-90e6-f1615a93659d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990128605310338\n",
      "\n",
      "0.9997984322730582\n",
      "\n",
      "0.9999988106436638\n",
      "\n",
      "0.99931233506996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feed_forward(input_to_second_layer_weights, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93a291-50a1-44eb-9596-27076eeb8316",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30741548-e3ac-433f-996d-25b255c62ba6",
   "metadata": {},
   "source": [
    "This is of course how the neural network works but there is a lot going on and how do we know what are weights and biases are. How is it trained, how do we fix the weight and biases if we have a lot of error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa9f0e-2881-4e9b-b0c7-c2d0fddc51dd",
   "metadata": {},
   "source": [
    "How does the cost function change with respect to weights, bias, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206560af-37ef-4f23-bce7-bee0988fc777",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PL_h2yd2CGtBHEKwEH5iqTZH85wLS-eUzv&index=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746dae94-bf23-417a-9ebb-36ed50506094",
   "metadata": {},
   "source": [
    "The way to figure this out is using back propagation. Back propagation is a algorithm that helps us figure out how we change the weights and biases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

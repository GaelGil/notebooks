{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274261cc-ff9f-4d9b-9f72-81ef0e65b5d7",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "In this notebook I will go over some of the basics of a fully connect feed forward neural network. I will mainly focus on their applications, how neural networks are formed, trained, and the math. To demonstrate this I will create my own neural network from scratch using `numpy` (a python library). \n",
    "\n",
    "First lets go over what data we are going to be using and what exactly we are trying to do. In this notebook I will use the use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) as an example. In this dataset we are given five columns `sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)` and `class`. The first four are all size measurements of a flowers sepal and petal. The `class` (label) column is used to represent if a flower is one of these three flowers `Iris-versicolor, Iris-setosa, Iris-virginica`. So now what we will attempt to do is classify each of these flowers given these measurements.\n",
    "\n",
    "Now that we know what this notebook is going go over I have to mention some things that will be helpful to know before reading. This notebook goes into the math of neural networks. Some math knowledge of matrix multiplication and multivariable calculus will be useful. We will be doing all the math with numpy so python is important as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26541d35-ee3e-404c-8101-42409b2771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0d811-41c6-4de2-b752-98f50ab9484d",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ef9278-e2e4-4b07-bce1-aac11da8e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "X_train = pd.read_csv(f'./{data_folder}/X_train.csv')\n",
    "y_train = pd.read_csv(f'./{data_folder}/y_train.csv')\n",
    "X_test = pd.read_csv(f'./{data_folder}/X_test.csv')\n",
    "y_test = pd.read_csv(f'./{data_folder}/y_test.csv')\n",
    "train_data = pd.concat([X_train, pd.get_dummies(y_train['class'])], axis=1)\n",
    "test_data = pd.concat([X_test, pd.get_dummies(y_test['class'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54daff26-8c49-4228-98d4-052d8fd10bdb",
   "metadata": {},
   "source": [
    "Taking a look at our data we can see that there are multiple variables. As I mentioned these variables are all size measurements of a flowers sepal and petal in cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7382ffc7-d7cf-4b20-92f9-04b7452715eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>Iris-setosa</th>\n",
       "      <th>Iris-versicolor</th>\n",
       "      <th>Iris-virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.5               2.4                3.7               1.0   \n",
       "1                  4.8               3.0                1.4               0.1   \n",
       "2                  5.5               2.6                4.4               1.2   \n",
       "3                  5.0               3.2                1.2               0.2   \n",
       "4                  6.9               3.1                5.1               2.3   \n",
       "..                 ...               ...                ...               ...   \n",
       "107                6.3               2.7                4.9               1.8   \n",
       "108                7.2               3.0                5.8               1.6   \n",
       "109                5.8               4.0                1.2               0.2   \n",
       "110                5.2               3.4                1.4               0.2   \n",
       "111                7.2               3.2                6.0               1.8   \n",
       "\n",
       "     Iris-setosa  Iris-versicolor  Iris-virginica  \n",
       "0              0                1               0  \n",
       "1              1                0               0  \n",
       "2              0                1               0  \n",
       "3              1                0               0  \n",
       "4              0                0               1  \n",
       "..           ...              ...             ...  \n",
       "107            0                0               1  \n",
       "108            0                0               1  \n",
       "109            1                0               0  \n",
       "110            1                0               0  \n",
       "111            0                0               1  \n",
       "\n",
       "[112 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8716d82a-6dfd-4f84-a542-72b169d77c84",
   "metadata": {},
   "source": [
    "Neural networks are very powerful tools that can help us solve problems with data that is non linear. For example if we have some data that is linear seperable we can easily draw a line to seperate and classify whatever it is we are trying to classify. But if we have some non linear data we can't do that. This is where neural networks come in. \n",
    "To further drive the point that simple logistic regression will not work we will graph the data to see what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4758fc-b766-4394-8563-d605022ae258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: graph with color\n",
    "plt.scatter(X_train['sepal length (cm)'], X_train['sepal width (cm)'])\n",
    "plt.xlabel('sepal length cm')\n",
    "plt.ylabel('sepal width cm')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f5d89-6b8e-4e88-a91d-bf30d8079819",
   "metadata": {},
   "source": [
    "Now that we have taken a look at our data we can get into creating a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6254fd-8c7c-48cf-8d3d-302e3e0a5049",
   "metadata": {},
   "source": [
    "# Neural Network Anatomy\n",
    "A neural network is formed by an input layer, some number of hidden layers, and an ouput layer. All of these layers contain some number of neurons. The input layer has as many neurons as it does inputs. For example in our network our input layer will have four neurons for `sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)`. Simiarly our output layer has as many neurons as it does classes. For example we have the three classes `Iris-versicolor, Iris-setosa, Iris-virginica` so we will have three output neurons. As I mentioned we can have as many hidden layers as we want. Within the hidden layers we can have as many neurons as we choose as well. To demonstrate what a nerual network looks like below is an image of the neural network that is made later in this notebook.\n",
    "\n",
    "The lines connecting the input layer to the first hidden layers are called weights same for the first hidden layer to the second hidden layers. These weights are what we will later be adjusting using back propogation.\n",
    "\n",
    "The idea of the nueral network is that we have some inputs and we want to pass them forward to through the network (from the input to hidden layers and to the output). The"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3cad9-74c3-495f-ba16-e363c8bd3eb6",
   "metadata": {},
   "source": [
    "# The Math (Feeding Forward)\n",
    "To feed forward an input from the input layer to the first hidden layer we will multiply all inputs $\\vec{x}$ by all the weights connecting the input layer to h1 $W_1$ and add some bias $b_1$. Doing all that will give us $h_1$. We then apply some non linearity function to that and that will be our activations. In this case we are using relu. This looks like this: \n",
    "$$ \\vec{z_1} = relu(\\vec{h_1})$$\n",
    "$$ \\vec{h_1} = W_1 * \\vec{x} + \\vec{b_1} $$\n",
    "$$ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$$\n",
    "\n",
    "Now we just continue this unitl we get to the output layer. So to keep forwarding this input we would now do this: \n",
    "\n",
    "$$ \\vec{z_2} = \\vec{h_2}$$\n",
    "$$ \\vec{h_2} = W_2 * \\vec{z_1} + \\vec{b_2} $$\n",
    "$$ \\vec{z_2} = W_2 * \\vec{z_1} + \\vec{b_2}$$\n",
    "\n",
    "And lastly our ouput will simply just be:\n",
    "$$ \\hat{y} = softmax(\\vec{z_2}) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9b866-5c46-44a6-a4ec-e0eee916eed6",
   "metadata": {},
   "source": [
    "# Back Propogation\n",
    "As I mentioned neural networks contain weights and biases. We can manually adjust them or set them to random variables to see which values give us the best result. This of course is not a good proccess seeing how our network has so many weights and biases. What we want to do instead is learn the values to our weights and biases that will minimise our loss. We can do this with calculus. We can do this by taking the negative gradient of our cost function ($ C = (\\hat{y}-y)^2$). The gradient gives the direction of fastest assent. So if we take the negative gradient we will approach a place in which our loss is as close to 0 as possible.  Below I will explain the math that allows us to learn the values for our weights and biases. This will involved mutlivariable calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009751c-da39-4b67-b374-e781e654b98d",
   "metadata": {},
   "source": [
    "This is a basic example of what our network looks like. We have an input($X$), two hidden layers ($h_1,h_2$) and two weights ($W_1,W_2$). \n",
    "$$ X ---W_1---> h_1 ---W_2---> h_2$$\n",
    "\n",
    "To calculate the error that our network produces we use the cost function $ C = (\\hat{y} - y)^2 $\n",
    "where $ \\hat{y} $ is the output vector/activations and $ y $ is the label as a vector. An example of what these look like is $ \\hat{y} = \\begin{bmatrix}\n",
    "0.8 \\\\\n",
    "0.1 \\\\\n",
    "0.1 \n",
    "\\end{bmatrix} $ and $ \\hat{y} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} $\n",
    "\n",
    "As I mentioned our output is $\\hat{y}$ which is given by the follwing function $\\hat{y} = softmax(h_2)$. The activations $h_2$ are passed trough the softmax function. \n",
    "\n",
    "$$ \\hat{y} = softmax( W_2 * \\vec{h_1} + \\vec{b_2} ) $$\n",
    "\n",
    "As we saw above to feed forward and input we use a series of functions. For example to get the first hidden layers activations we do this $ \\vec{z_1} = relu(W_1 * \\vec{x} + \\vec{b_1})$. What we want to do now is see how much each weight and bias is affecting our cost. In other words whats the derivative of the cost function $C$ with respect to $W_1$. \n",
    "\n",
    "In the example below we will take the derivative of our cost function with respect to the second weights ($W_2$).\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "\n",
    "$$ z^\\hat{y} = W_2 * \\vec{z_1} + \\vec{b_2}  \\hspace{10mm}  \\frac{\\partial z^\\hat{y}}{\\partial W_2} = \\vec{z_1}$$\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial z^\\hat{y}}{\\partial W_2} = 2(\\hat{y} - y) * \\vec{h_1} $$\n",
    "\n",
    "This will tell us by how much we should change the weights in $W_2$ Now we just continue this proccess until we reach the input layer. But first we need to calculate the error produced by the activations in the first hidden layer $\\frac{\\partial C}{\\partial h_1}$.\n",
    "\n",
    "$$ C = (\\hat{y}-y)^2  \\hspace{10mm} \\frac{\\partial C}  {\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "$$ \\hat{y} = h_2  \\hspace{10mm} \\frac{\\partial \\hat{y}}  {\\partial h_2}$$\n",
    "$$ h_2 = W_2 *z_1 + b_1 \\hspace{10mm} \\frac{\\partial h_2}  {\\partial z_1} = W_2$$\n",
    "$$ z_1 = relu(h_1) \\hspace{10mm} \\frac{\\partial z_1}  {\\partial h_1} = relu`(h_1)$$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial h_1} = \\frac{\\partial C}  {\\partial \\hat{y}} \\frac{\\partial \\hat{y}}  {\\partial h_2} \\frac{\\partial h_2}  {\\partial z_1} \\frac{\\partial z_1}  {\\partial h_1} = 2(\\hat{y} - y) *  W_2 * relu`(h_1)$$\n",
    "\n",
    "$$ 2(\\hat{y} - y) *  W_2 * relu`(h_1) $$ This will tell us the error in the first hidden layer. Now we can continue our back propogation. Our next derivative to get is going to be $\\frac{\\partial h_1}{\\partial W_1}$. This will tell use how to change the weights in $W_1$. Because we have calculated all of our chain rule deriatives all that is left is just now.\n",
    "\n",
    "$$ h_1 = \\vec{x}*W_1+b_1 \\hspace{10mm} \\frac{\\partial h_1}  {\\partial W_1} = \\vec{x}$$\n",
    "\n",
    "$$ \\frac{\\partial C}  {\\partial h_1} \\frac{\\partial h_1}  {\\partial W_1}  = \\vec{x}\\frac{\\partial C}  {\\partial h_1} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465205f8-0be9-4c4e-b096-11d97e29aae9",
   "metadata": {},
   "source": [
    "Now that we have talked about how a neural network feeds forward an input to get and output. And that we have went over how we can learn the values of the weights and biases of our neural network, it is time to build one. Below is the code to a neural network class which we will train to classify dataset. The network below has an input layer of four nerons for our for inputs, 2 hidden layers one of two neurons and one of 3 output neurons. I will go over how a neural network is trained below as well but as mentioned it involves the calculus that is explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e80ed6-9b67-4a33-baff-322cc4200ab2",
   "metadata": {},
   "source": [
    "# Creating the Network\n",
    "Below is the code for a neural network. As explained earlier our network will have a input layer, two hidden layers, and two weights. It has a function to feed forward an input and also to perform back propagation. Lastly it also has a function to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed078904-669e-48d0-a372-ae6da15c247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        # layers/activations\n",
    "        self.input = np.random.rand(4, 1)\n",
    "        self.h1 = np.random.rand(2, 1)\n",
    "        self.h2 = np.random.rand(3, 1)\n",
    "        \n",
    "        # weights \n",
    "        self.w_1 = np.random.rand(2, 4)\n",
    "        self.w_2 = np.random.rand(3, 2)\n",
    "        \n",
    "        # biases \n",
    "        self.b_1 = np.random.rand(2, 1)\n",
    "        self.b_2 = np.random.rand(3, 1)\n",
    "\n",
    "    def relu(self,activations):\n",
    "        return np.maximum(0, activations)\n",
    "\n",
    "    def relu_deriv(self, activations):\n",
    "        return activations > 0\n",
    "\n",
    "    def softmax(self, activations):\n",
    "        return np.exp(activations) / np.sum(np.exp(activations))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        # input\n",
    "        # reshapre the input into vector form.\n",
    "        # Example [1, 0, 0] -> [[1], [1], [1]]\n",
    "        self.input = np.reshape(X, (-1,1))\n",
    "        \n",
    "        # input -> h1\n",
    "        # the activations in the first hidden layer are given by the dot product \n",
    "        # of the weights by the input plus some biass its all then passed into\n",
    "        # our activation function. relu(W_1*x+b_1)\n",
    "        h1_activations = self.relu(np.dot(self.w_1, self.input) + self.b_1)\n",
    "        self.h1 = h1_activations\n",
    "\n",
    "        # h1 -> h2\n",
    "        # the activations in the seocnd hidden layer (h_2) are given by the dot product \n",
    "        # of the second weights (w_2) by the previous activations (h1) plus the bias(b_2).\n",
    "        # W_2*h1+b_2\n",
    "        h2_activations = (np.dot(self.w_2, h1_activations) + self.b_2)\n",
    "        self.h2 = h2_activations \n",
    "\n",
    "        # h2 -> output \n",
    "        # our output activtions/predictions are given by the second layer activations (h_2)\n",
    "        # put into the softmax function. \n",
    "        output = self.softmax(self.h2)\n",
    "        \n",
    "        return (output, max(output))\n",
    "    \n",
    "    def back_prop(self, output, y, learning_rate=0.01):\n",
    "        y = np.reshape(y, (-1,1))\n",
    "        \n",
    "        output_error = (2 * (output - y))\n",
    "        \n",
    "        dw_2 = -learning_rate * output_error.dot(self.h1.T)\n",
    "        b_2 = -learning_rate * output_error\n",
    "\n",
    "        h1_error = self.w_2.T.dot(output_error) * self.relu_deriv(self.h1)\n",
    "        dw_1 = -learning_rate * h1_error.dot(self.input.T)\n",
    "        b_1 = -learning_rate * h1_error\n",
    "        \n",
    "        # update all the weights\n",
    "        self.w_1 +=  dw_1\n",
    "        self.w_2 +=  dw_2\n",
    "        \n",
    "        # update all the biases\n",
    "        self.b_1 += b_1\n",
    "        self.b_2 += b_2\n",
    "        return 0\n",
    "       \n",
    "    def accuracy_and_cost(self, data):\n",
    "        total_correct = 0\n",
    "        cost = 0\n",
    "        for index, row in data.iterrows():\n",
    "            # Our input value is the first four colmuns in our dataset.\n",
    "            # Since each row is turned into a list here we select the\n",
    "            # first four elements in the list whihch will be our intput.\n",
    "            X = row.tolist()[:4] \n",
    "            # Our output value is the last three colmuns in our dataset.\n",
    "            # To select it we select the last three elemets in the list\n",
    "            # that will be our output.\n",
    "            y = row.tolist()[4:]\n",
    "            output, predicted = self.feed_forward(X)\n",
    "            # get the index of the label (ie index = 2 [0, 0, 1])\n",
    "            label_index = y.index(max(y))\n",
    "            # check if the index of the label is the same as our\n",
    "            # index as the max value in our output vector\n",
    "            predicted_index = np.where(output==predicted)[0][0]\n",
    "            # if they are the same index increase the correct counter\n",
    "            if label_index == predicted_index:\n",
    "                total_correct += 1\n",
    "            cost += (1-predicted)**2\n",
    "        return total_correct/data.shape[0], cost\n",
    "    \n",
    "    def train(self, data, iterations):\n",
    "        for i in range(iterations):\n",
    "            for index, row in data.iterrows():\n",
    "                # Our input value is the first four colmuns in our dataset.\n",
    "                # Since each row is turned into a list here we select the\n",
    "                # first four elements in the list whihch will be our intput.\n",
    "                X = row.tolist()[:4] \n",
    "                # Our output value is the last three colmuns in our dataset.\n",
    "                # To select it we select the last three elemets in the list\n",
    "                # that will be our output.\n",
    "                y = row.tolist()[4:]\n",
    "                output, predicted = self.feed_forward(X)\n",
    "                self.back_prop(output, y)\n",
    "        cost, training_accuracy = self.accuracy_and_cost(data)\n",
    "        return training_accuracy, cost\n",
    "    \n",
    "    def get_loss(self, X, y):\n",
    "        output, predicted = self.feed_forward(X)\n",
    "        return (1-predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39db34c-512b-4caa-aeed-2cebe509dc2d",
   "metadata": {},
   "source": [
    "# Using the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08957323-14f7-4c32-92c8-4567b14267a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = [1, 5, 10, 50, 100, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a227d2cc-1c5a-4dd0-b85d-09f1ef028ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_info(iterations, train_data, test_data):\n",
    "    \"\"\"\n",
    "    This function will return data to graph how our neural network\n",
    "    changes as we increas the iterations. \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['iteration', 'training_cost', 'test_cost'])\n",
    "    for i in iterations:\n",
    "        # create a new network\n",
    "        network = Network()\n",
    "        # train our network (also return trainin accuracy)\n",
    "        training_cost, training_accuracy = network.train(train_data, i)\n",
    "        # returns test accuracy\n",
    "        test_cost, test_accuracy = network.accuracy_and_cost(test_data)\n",
    "        # add to df\n",
    "        df.loc[i] = [i] + [training_cost] + [test_cost]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799fcd8e-967f-498f-9721-16a0de7fe53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gael/git-repos/notebooks/back-propagation/env/lib/python3.8/site-packages/pandas/core/dtypes/cast.py:881: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  element = np.asarray(element)\n"
     ]
    }
   ],
   "source": [
    "df = neural_network_info(iterations, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8816335-fb9e-4151-b25b-4f21108c9315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>training_cost</th>\n",
       "      <th>test_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[23.951257182124372]</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[4.301418591385835]</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[3.948168095758673]</td>\n",
       "      <td>0.763158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>[1.7482728254647806]</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>[0.7954001712006808]</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>[0.19801931958053334]</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1000</td>\n",
       "      <td>[0.21813801023669588]</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>1500</td>\n",
       "      <td>[0.28731250876520137]</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>[0.39192679156973703]</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>2500</td>\n",
       "      <td>[0.46485728474530874]</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>3000</td>\n",
       "      <td>[0.4813988716138651]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>3500</td>\n",
       "      <td>[0.5148288968966979]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>4000</td>\n",
       "      <td>[0.283965511992251]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iteration          training_cost  test_cost\n",
       "1            1   [23.951257182124372]   0.578947\n",
       "5            5    [4.301418591385835]   0.631579\n",
       "10          10    [3.948168095758673]   0.763158\n",
       "50          50   [1.7482728254647806]   0.921053\n",
       "100        100   [0.7954001712006808]   0.921053\n",
       "500        500  [0.19801931958053334]   0.947368\n",
       "1000      1000  [0.21813801023669588]   0.947368\n",
       "1500      1500  [0.28731250876520137]   0.947368\n",
       "2000      2000  [0.39192679156973703]   0.947368\n",
       "2500      2500  [0.46485728474530874]   0.947368\n",
       "3000      3000   [0.4813988716138651]   1.000000\n",
       "3500      3500   [0.5148288968966979]   1.000000\n",
       "4000      4000    [0.283965511992251]   1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df41cc7d-c16d-4cdc-8596-2594b5b51334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['iteration'], df['training_cost'], color='b', label='training cost')\n",
    "# plt.scatter(df['iteration'], df['test_cost'], color='b', label='test cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cost')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Accuracy by Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c71f4d5-4105-4313-a5fe-24ffdcee7e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8cfead-051a-454b-8eca-77d714d7c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cbc3e8c-79ed-4242-8d4d-8de4c7b0fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_accuracy = nn.train(data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a260a947-45c6-4d63-a721-7783cb7908fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Training Accuracy: {training_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "992bdbbb-1015-41c2-8d54-44b18ea6ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Test Accuracy: {nn.get_accuracy(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d552b-eff8-4086-8b62-f2dd0e13a2c9",
   "metadata": {},
   "source": [
    "# Explaning Further Topics of Neural Networks \n",
    "- > dropput\n",
    "- > training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6b1ef-9e75-4ae8-9f39-e0897a9a59b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3055ed1-11ee-4427-8302-4d217bea94a6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4576778-e372-4fba-8ee6-6a715519f2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
